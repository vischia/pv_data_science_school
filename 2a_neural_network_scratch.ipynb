{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/pv_data_science_school/blob/master/2a_neural_network_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning School, ICNFP 2025 edition\n",
    "## Exercise 2a: write your own neural network from scratch\n",
    "\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will analyze the \"Housing\" dataset used in the first PAs, consisting in data on a set of houses in San Francisco and New York. The dataset is available through https://github.com/jadeyee/r2d3-part-1-data .\n",
    "\n",
    "The goal is to classify houses as being New York or San Francisco houses.\n",
    "\n",
    "Let's load the data, using the same commands as in the other notebooks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_sf</th>\n",
       "      <th>beds</th>\n",
       "      <th>bath</th>\n",
       "      <th>price</th>\n",
       "      <th>year_built</th>\n",
       "      <th>sqft</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999000</td>\n",
       "      <td>1960</td>\n",
       "      <td>1000</td>\n",
       "      <td>999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2750000</td>\n",
       "      <td>2006</td>\n",
       "      <td>1418</td>\n",
       "      <td>1939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1350000</td>\n",
       "      <td>1900</td>\n",
       "      <td>2150</td>\n",
       "      <td>628</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>629000</td>\n",
       "      <td>1903</td>\n",
       "      <td>500</td>\n",
       "      <td>1258</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>439000</td>\n",
       "      <td>1930</td>\n",
       "      <td>500</td>\n",
       "      <td>878</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_sf  beds  bath    price  year_built  sqft  price_per_sqft  elevation\n",
       "0      0   2.0   1.0   999000        1960  1000             999         10\n",
       "1      0   2.0   2.0  2750000        2006  1418            1939          0\n",
       "2      0   2.0   2.0  1350000        1900  2150             628          9\n",
       "3      0   1.0   1.0   629000        1903   500            1258          9\n",
       "4      0   0.0   1.0   439000        1930   500             878         10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget https://raw.githubusercontent.com/jadeyee/r2d3-part-1-data/refs/heads/master/part_1_data.csv\n",
    "    \n",
    "data = pd.read_csv(\"./part_1_data.csv\", sep=\",\", skiprows=2)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_sf</th>\n",
       "      <th>beds</th>\n",
       "      <th>bath</th>\n",
       "      <th>price</th>\n",
       "      <th>year_built</th>\n",
       "      <th>sqft</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.092123</td>\n",
       "      <td>0.168321</td>\n",
       "      <td>0.042017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.093821</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.147931</td>\n",
       "      <td>0.385361</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.042562</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.245661</td>\n",
       "      <td>0.082660</td>\n",
       "      <td>0.037815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016164</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>0.025367</td>\n",
       "      <td>0.228123</td>\n",
       "      <td>0.037815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.025367</td>\n",
       "      <td>0.140383</td>\n",
       "      <td>0.042017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_sf  beds      bath     price  year_built      sqft  price_per_sqft  \\\n",
       "0    0.0   0.2  0.000000  0.029711    0.588235  0.092123        0.168321   \n",
       "1    0.0   0.2  0.111111  0.093821    0.926471  0.147931        0.385361   \n",
       "2    0.0   0.2  0.111111  0.042562    0.147059  0.245661        0.082660   \n",
       "3    0.0   0.1  0.000000  0.016164    0.169118  0.025367        0.228123   \n",
       "4    0.0   0.0  0.000000  0.009208    0.367647  0.025367        0.140383   \n",
       "\n",
       "   elevation  \n",
       "0   0.042017  \n",
       "1   0.000000  \n",
       "2   0.037815  \n",
       "3   0.037815  \n",
       "4   0.042017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "#data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "\n",
    "\n",
    "for c in data.columns:\n",
    "    data[c] = scaler.fit_transform(data[c].to_numpy().reshape(-1, 1))\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into a training and a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 329 training samples with 184 signal and 145 background events\n",
      "We have 163 testing samples with 84 signal and 79 background events\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "try:\n",
    "    # See #1137: this allows compatibility for scikit-learn >= 0.24\n",
    "    from sklearn.utils import safe_indexing\n",
    "except ImportError:\n",
    "    from sklearn.utils import _safe_indexing\n",
    "    \n",
    "    \n",
    "X = data.drop([\"in_sf\"], axis=1)\n",
    "y = data[\"in_sf\"]\n",
    "\n",
    "\n",
    "import sklearn\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"We have {len(X_train)} training samples with {sum(y_train)} signal and {sum(1-y_train)} background events\")\n",
    "print(f\"We have {len(X_test)} testing samples with {sum(y_test)} signal and {sum(1-y_test)} background events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset ready, it's time to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code a neural network from scratch\n",
    "\n",
    "#### Details on neural networks\n",
    "\n",
    "Biology teaches us that the brain is constituted of neurons and connections between them: the synapses.\n",
    "By comparing the brain of various animals, we now think that the more the number of neurons and most importantly of synapses is large, the more complex are the functions that the brain can execute.\n",
    "\n",
    "Let's learn the inner workings of a very simplified mathematical model of brain: an artificual neural network.\n",
    "\n",
    "The first element is the neuron. The simplest model (and one of the first) we have is the [*perceptron*](https://en.wikipedia.org/wiki/Perceptron). The neuron is modelled by a mathematical function that takes some arguments as inputs, combines them linearly, and returns an output value.\n",
    "We denote as *weights* the coefficients of the linear combination.\n",
    "\n",
    "However, we want to be able to approximate nonlinear functions, so we need to plug in a degree of nonlinearity inside the neuron, and we want the neuron to fire only when a certain threshold in the output is reached (a certain amount of stimulation).\n",
    "\n",
    "We modify the output of the neuron by an activation function $f_{act}$: the neuron is activated if the activation function returns a non-zero value. The output of the neuron is defined as:\n",
    "\n",
    "$$\n",
    "y_n = f_{act}(\\sum_i w_{i,n} x_{i,n})\n",
    "$$\n",
    "\n",
    "If the activation function is not linear, we are happy because we have obtained a neuron that gives a nonlinear output and gets activated only if the stimuli it receives are large enough.\n",
    "\n",
    "You will use in this exercise two activation functions:\n",
    "\n",
    "- [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), a function $f(x)$ that returns 0 if $x<0$, and $x$ otherwise. It is used to introduce a nonlinearity while allowing the variable to have a wide range of values;\n",
    "- [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), a function that rescales any number into a number between 0 and 1. It is often used in classification contexts as the activation function for the output layer, to be able to interpret outputs as probabilities.\n",
    "\n",
    "Here you have a graphical representation of the perceptron, by [https://towardsdatascience.com](https://towardsdatascience.com):\n",
    "\n",
    "![neuron](https://miro.medium.com/max/1435/1*n6sJ4yZQzwKL9wnF5wnVNg.png \"Figure from https://towardsdatascience.com\")\n",
    "\n",
    "Now we have to connect the neurons. The simplest way is to build layers of neurons, and to connect all neurons of consecutive layers. Starting with the inputs, there is a first layer of neurons. Each neuron combines linearly the inputs and passes the result through activation function to give an output value. The set of outputs of a layer will be the input of the following layer:\n",
    "\n",
    "![neuralnet](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/800px-Colored_neural_network.svg.png \"Figure from wikipedia\")\n",
    "\n",
    "A neural network is characterized by a set of weights assigned to the connections that define the structure of the network. You can see this as a mathematical function with many free parameters (the weights) that takes the inputs and gives an output. The problem of learning is then the problem of finding the values of the free parameters that minimize the difference between the output and the target distribution that we want to learn.\n",
    "\n",
    "#### The loss function\n",
    "\n",
    "For regression problems, the loss function is typically the Mean Square Error (MSE).\n",
    "\n",
    "For classification problems, the loss function is typically the *binary cross entropy*, defined for one data point $i$ as:\n",
    "\n",
    "$$\n",
    "Loss_i = -( y_{true}log(\\hat{y}) + (1-y_{true})log(1-\\hat{y}) )\n",
    "$$\n",
    "\n",
    "ATTENTION: if $\\hat{y}$ is either 0 or 1, the log is undefined. You can add a workaround by putting log(0) to minus infinity, e.g., if $\\hat{y}$ is defined as $yhat$, `(-10^8 if (yhat==0 or yhat==1) else np.log(yhat) )`\n",
    "\n",
    "\n",
    "#### The training process\n",
    "\n",
    "Schematically, the training process consists in:\n",
    "\n",
    "- for each epoch\n",
    "   * for each training set data point:\n",
    "      1. calculate the output of each neuron, starting from the inputs to the output\n",
    "      2. compare the output of the last neuron with the reference wine quality\n",
    "      3. propagate the error back towards the inputs, without updating the weights (the error needs to be propagated with respect to the current values of the weights)\n",
    "      4. update all the weights\n",
    "      5. save the value of the loss function for each event\n",
    "   * for each test set data point:\n",
    "      1. save the value of the loss function for each event\n",
    "   * aggregate the loss function by computing an average\n",
    "      1. For the training dataset, this is the average training loss\n",
    "      2. For the test dataset, this is the average validation loss (you see here I am using validation and test indifferently)\n",
    "\n",
    "The idea is the training will stop when the loss function doesn't improve anymore (it remains stationary at its minimum. If the training loss keeps diminishing and the test loss begins increasing, then we might be starting to learn statistical fluctuations of the training dataset.\n",
    "\n",
    "\n",
    "#### Clarification on the connections between networks (to fix ideas)\n",
    "\n",
    "If the network has the following structure: (input layer: two inputs `A` et `B`;  first internal (_hidden_) layer: two neurons `1a` et `1b`; second hidden layer: two neurons `2a` et `2b`; output `y`), the list of connections (the weights) is:\n",
    "\n",
    "- Four weights connecting the inputs to the layer 1:\n",
    "    - `wA1a` (connects input `A` to neuron `1a`)\n",
    "    - `wA1b` (connects input `A` to neuron `1b`)\n",
    "    - `wB1a` (connects input `B` to neuron `1a`)\n",
    "    - `wB1b` (connects input `B` to neuron `1b`)\n",
    "- Four weights connecting the neurons of layer 1 to those of layer 2:_\n",
    "    - `W1a2a` (connects neuron `1a` to neuron `2a`)\n",
    "    - `W1a2b` (connects neuron `1a` to neuron `2b`)\n",
    "    - `W1b2a` (connects neuron `1b` to neuron `2a`)\n",
    "    - `W1b2b` (connects neuron `1b` to neuron `2b`)\n",
    "- Two weights connecting the neurons of layer 2 to the output y:\n",
    "    - `W2ay` (connects neuron `2a` to output `y`)\n",
    "    - `W2by` (connects neuron `2b` to output `y`)\n",
    "\n",
    "####  Backpropagation\n",
    "\n",
    "To perform backpropagation we need, for each neuron, to propagate back the error of the neurons of the following layer (so you need to go backwards). We use the chain rule.\n",
    "\n",
    "- Error for a neuron of the output layer:\n",
    "\n",
    "$$\n",
    "\\epsilon = (y_{true} - \\hat{y}) * activation\\_derivative(\\hat{y})\n",
    "$$\n",
    "\n",
    "Here $\\hat{y}$ is the output of this output neuron, and $y_{true}$ is the target quality of the wine\n",
    "\n",
    "- Error for a neuron $m$ of an internal layer $N$:\n",
    "\n",
    "$$\n",
    "\\epsilon_{m, N} = \\sum_{k} (w_{k, N+1} * \\epsilon_{k, N+1}) * activation\\_derivative(\\hat{m})\n",
    "$$\n",
    "\n",
    "Here, $\\epsilon_{k,N+1}$ is the error of the neuron $k$ of the following layer (layer $N+1$), $w_{k, N+1}$ is the weight of the connection between the neuron $m$ and the neuron $k$ of the next layer, and $\\hat{m}$ is the output of neuron $m$\n",
    "\n",
    "\n",
    "#### Updating the weights\n",
    "\n",
    "After having backpropagated all the gradient, you have to update all the weights using this formula:\n",
    "\n",
    "$$\n",
    "w = w + learning\\_rate * error * input\n",
    "$$\n",
    "\n",
    "Ici $error$ is the error calculated via backpropagation, $input$ is the input value of the neuron that had been originally passed to the neuron, and $learning rate$ is a parameter governing how fast we climb down the gradient.\n",
    "\n",
    " \n",
    "#### At the end of each epoch\n",
    "\n",
    "To check for convergence of the network, a standard practice is to aggregate the errors $\\hat{y} - y_{true}$  of all the events at the end of each epoch, in order to reduce the sensitivity to statistical fluctuations in the training sample. The first pillar of statistical wisdom according to Stigler is precisely aggregation. \n",
    "\n",
    "For regression problems, in analogy with $\\chi^2$ fit, we can for example calculate the $MSE = \\frac{1}{N} \\sum_{events} (\\hat{y}-y_{true})^2$ and plot the MSE as a function of the number of epochs. If the network is improving its predictions, we should see something like this:\n",
    "\n",
    "![mse](https://cern.ch/vischia/mse_pythonCourse.png \"Figure by Pietro Vischia, 2019\")\n",
    "\n",
    "For classification problems, the figure of merit is typically the accuracy, defined as:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TruePositives + TrueNegatives}{TruePositives+TrueNegatives+FalsePositives+FalseNegatives}\\,.\n",
    "$$\n",
    "\n",
    "*HINT*: you can calculate these quantities easily from the confusion matrix.\n",
    "\n",
    "\n",
    "#### Diagnostic plots\n",
    "\n",
    "- 1) Average loss as a function of the epoch\n",
    "- 2a) Regression: histogram of $\\frac{\\hat{y} - y_{true}}{y_{true}}$ \n",
    "- 2b) Regression: histogram of $\\frac{\\hat{y} - y_{true}}{y_{true}}$ as a function of $y_{true}$\n",
    "- 3a) Classification: confusion matrix\n",
    "- 3b) Classification: ROC curve\n",
    "\n",
    "### Weights initialization\n",
    "- To initialize the weights at the beginning you can use a Gaussian, or a truncated gaussian ( (scipy.stats.truncnorm), or a $random uniform[0,1]$\n",
    "\n",
    "\n",
    "### Which gradient descent scheme should you check?\n",
    "\n",
    "All three :D\n",
    "\n",
    "The code should take as an input the scheme: `stochastic`, `batch`, `minibatch`. For `minibatch`, an additional parameter should be taken as an input, i.e. the number of events per batch.\n",
    "\n",
    "- `stochastic`: the event weights are updated after backpropagating each data point. One epoch corresponds to having iterated through all training events (i.e. one epoch corresponds to $N_{train}$ weight updates)\n",
    "- `batch`: the event weights are updated only after all training events have been processed. One epoch corresponds to having iterated through all training events (i.e. one epoch corresponds to $1$ weight update)\n",
    "- `minibatch`: a relatively small number of events (a batch) is sampled each time, and weights are updated after every batch (one epoch is defined as $N_{train}/batchSize$ iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start with the elements we need.\n",
    "\n",
    "First, we need to realize that although we could in principle model the problem in an object-oriented way, that is define a class `Neuron` and have a class `Network` that collects neurons maybe into instances of a class `Layer`, this doesn't exploit the synthetical concepts behind representing neural network operations in a backgpropagation-friendly way.\n",
    "\n",
    "In other words, the simplest way we can encode the problem is to have in our mind the neural structure as a set of neurons with connections, and write instead in the code the problem as a set of weights that must be combined in ways defined by the neurons.\n",
    "\n",
    "This highlights the fact that the neuron outputs are transient: the real parameters of the network, that we must store for learning and later use, are the weights.\n",
    "\n",
    "All of this to say that we will just store appropriately-sized matrices of weights, and the neurons wil \"exist\" as operations of matrix multiplication between the weights matrices.\n",
    "\n",
    "To make matrix multiplication we will use `np.dot`.\n",
    "\n",
    "The exact implementation is courtesy https://www.analyticsvidhya.com/blog/2020/07/neural-networks-from-scratch-in-python-and-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a: 2x3\n",
    "a=[\n",
    "    [2,2,3],\n",
    "    [2,2,3]\n",
    "]\n",
    "# b: 3x4\n",
    "b=[\n",
    "   [1,2,3,4],\n",
    "   [1,2,3,4],\n",
    "   [1,2,3,4]\n",
    "]\n",
    "\n",
    "# a dot b: 2x4\n",
    "\n",
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to write our activation function and its derivative.\n",
    "\n",
    "Let's implement the sigmoid (we will use it for the output layer).\n",
    "You can also implement the ReLU if you wish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):    \n",
    "    return 1. / (1. + np.exp(-input))\n",
    "\n",
    "# Define the sigmoid derivative function\n",
    "def sigmoid_derivative(input):\n",
    "    return sigmoid(input) * (1.0 - sigmoid(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-10,10,100)\n",
    "\n",
    "plt.plot(x, sigmoid(x), label='sigmoid(x)')\n",
    "plt.plot(x, sigmoid_derivative(x), label='sigmoid\\'(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function become larger than 0.5 at $x=0$. We need to include the possibility of shifting the value for which the neuron activates. This is done by introducing a bias.\n",
    "\n",
    "The neuron output from\n",
    "\n",
    "$$\n",
    "y = f(\\sum(w_i x_i))\n",
    "$$\n",
    "\n",
    "(that activates for $\\sum(w_i x_i)>0$) will be\n",
    "\n",
    "$$\n",
    "y = f(\\sum(w_i x_i+ w_bb) )\n",
    "$$\n",
    "\n",
    "that activates at a learnable ($w_b$) value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define what's the structure of the network.\n",
    "\n",
    "Let's assume we have one input layer,  one hidden layer, and one output layer. We now define the number of neurons per each layer. You can generalize to an array of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = x_train.shape[0]\n",
    "n_hidden = 20  \n",
    "n_output = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to create the arrays for the weights and initialize the weights to a starting value, for the first forward pass. The weights will be later updated.\n",
    "\n",
    "Remember to set appropriate sizes to account for the dot products.\n",
    "\n",
    "You can later try to see what changes in the training when initializing e.g. with gaussian weights.\n",
    "\n",
    "Let's also disregard the bias :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From input layer to hidden layer\n",
    "weights_input_hidden = np.random.normal(0, 1, size=(n_inputs, n_hidden))\n",
    "# From hidden layer to output layer\n",
    "weights_hidden_output = np.random.normal(0, 1, size=(n_hidden, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the forward pass, for each layer transition.\n",
    "\n",
    "We can also print out what happens for an untrained forward pass, and the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating hidden layer activations\n",
    "hiddenLayer_linearTransform = np.dot(weights_input_hidden.T, x_train)\n",
    "hiddenLayer_activations = sigmoid(hiddenLayer_linearTransform)\n",
    "\n",
    "# calculating the output\n",
    "outputLayer_linearTransform = np.dot(weights_hidden_output.T, hiddenLayer_activations)\n",
    "output = sigmoid(outputLayer_linearTransform)\n",
    "\n",
    "\n",
    "\n",
    "print('MSE:', np.sum(np.square(y_train - output))/len(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the backpropagation algorithm. We start with the rate of change of the output error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.square(y_train - output) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's proceed through the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# calculating rate of change of error w.r.t weight between hidden and output layer\n",
    "error_wrt_output = -(y_train - output)\n",
    "output_wrt_outputLayer_LinearTransform = np.multiply(output, (1 - output))\n",
    "outputLayer_LinearTransform_wrt_weights_hidden_output = hiddenLayer_activations\n",
    "\n",
    "error_wrt_weights_hidden_output = np.dot(\n",
    "            outputLayer_LinearTransform_wrt_weights_hidden_output,\n",
    "        (error_wrt_output * output_wrt_outputLayer_LinearTransform).T,\n",
    "        )\n",
    "\n",
    "# calculating rate of change of error w.r.t weights between input and hidden layer\n",
    "outputLayer_LinearTransform_wrt_hiddenLayer_activations = weights_hidden_output\n",
    "hiddenLayer_activations_wrt_hiddenLayer_linearTransform = np.multiply(\n",
    "            hiddenLayer_activations, (1 - hiddenLayer_activations)\n",
    "    )\n",
    "hiddenLayer_linearTransform_wrt_weights_input_hidden = x_train\n",
    "error_wrt_weights_input_hidden = np.dot(\n",
    "        hiddenLayer_linearTransform_wrt_weights_input_hidden,\n",
    "        (\n",
    "            hiddenLayer_activations_wrt_hiddenLayer_linearTransform\n",
    "            * np.dot(\n",
    "                outputLayer_LinearTransform_wrt_hiddenLayer_activations,\n",
    "                (output_wrt_outputLayer_LinearTransform * error_wrt_output),\n",
    "            )\n",
    "        ).T,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "\n",
    "weights_hidden_output = weights_hidden_output - lr * error_wrt_weights_hidden_output\n",
    "weights_input_hidden = weights_input_hidden - lr * error_wrt_weights_input_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have described fully the process.\n",
    "\n",
    "Let's put it together in a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "lr=0.01\n",
    "epochs=1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ## Forward Propogation\n",
    "\n",
    "    # calculating hidden layer activations\n",
    "    hiddenLayer_linearTransform = np.dot(weights_input_hidden.T, x_train)\n",
    "    hiddenLayer_activations = sigmoid(hiddenLayer_linearTransform)\n",
    "\n",
    "    # calculating the output\n",
    "    outputLayer_linearTransform = np.dot(\n",
    "        weights_hidden_output.T, hiddenLayer_activations\n",
    "    )\n",
    "    output = sigmoid(outputLayer_linearTransform)\n",
    "\n",
    "    ## Backward Propagation\n",
    "\n",
    "    # calculating error\n",
    "    #error_blah = np.square(y_train - output) / 2\n",
    "    mylog = lambda x: (-10^8 if (x==0 or x==1) else np.log(x) )\n",
    "    error_blah = -( y_train * np.log(output) + (1-y_train)*np.log(1-output)) \n",
    "    \n",
    "\n",
    "    # calculating rate of change of error w.r.t weight between hidden and output layer\n",
    "    error_wrt_output = -(y_train - output)\n",
    "    output_wrt_outputLayer_LinearTransform = np.multiply(output, (1 - output))\n",
    "    outputLayer_LinearTransform_wrt_weights_hidden_output = hiddenLayer_activations\n",
    "\n",
    "    error_wrt_weights_hidden_output = np.dot(\n",
    "        outputLayer_LinearTransform_wrt_weights_hidden_output,\n",
    "        (error_wrt_output * output_wrt_outputLayer_LinearTransform).T,\n",
    "    )\n",
    "\n",
    "    # calculating rate of change of error w.r.t weights between input and hidden layer\n",
    "    outputLayer_LinearTransform_wrt_hiddenLayer_activations = weights_hidden_output\n",
    "    hiddenLayer_activations_wrt_hiddenLayer_linearTransform = np.multiply(\n",
    "        hiddenLayer_activations, (1 - hiddenLayer_activations)\n",
    "    )\n",
    "    hiddenLayer_linearTransform_wrt_weights_input_hidden = x_train\n",
    "    error_wrt_weights_input_hidden = np.dot(\n",
    "        hiddenLayer_linearTransform_wrt_weights_input_hidden,\n",
    "        (\n",
    "            hiddenLayer_activations_wrt_hiddenLayer_linearTransform\n",
    "            * np.dot(\n",
    "                outputLayer_LinearTransform_wrt_hiddenLayer_activations,\n",
    "                (output_wrt_outputLayer_LinearTransform * error_wrt_output),\n",
    "            )\n",
    "        ).T,\n",
    "    )\n",
    "\n",
    "    # updating the weights\n",
    "    weights_hidden_output = weights_hidden_output - lr * error_wrt_weights_hidden_output\n",
    "    weights_input_hidden = weights_input_hidden - lr * error_wrt_weights_input_hidden\n",
    "    # print error at every 100th epoch\n",
    "    epoch_loss = np.average(error_blah)\n",
    "    if epoch%100==0:\n",
    "        print(f\"Error at epoch {epoch} is {epoch_loss}\")\n",
    "\n",
    "    # appending the error of each epoch\n",
    "    losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "- implement the algorithm packing it in functions for training and test, extract the predictions from this network, plot the output and the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
