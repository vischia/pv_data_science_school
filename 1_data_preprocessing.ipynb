{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/pv_data_science_school/blob/master/1_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning School, ICNFP 2025 edition\n",
    "## Lesson 1: Data Preprocessing and Model Diagnostics\n",
    "\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few global settings\n",
    "\n",
    "- If you run on colab, set to `True` the relevant environmental variable.\n",
    "\n",
    "This tutorial will assume you have a few GB free wherever you are running (locally on your machine, or in your google drive account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runOnColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runOnColab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd \"/content/drive/MyDrive/\"\n",
    "    if not os.path.isdir(\"pv_data_science_schoool\"): \n",
    "        %git clone https://github.com/vischia/pv_data_science_school.git\n",
    "    %cd pv_data_science_school\n",
    "#!pwd\n",
    "#!ls\n",
    "##!pip install livelossplot shap\n",
    "#%pip install shap torchinfo livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "#import re\n",
    "#import math\n",
    "#import socket\n",
    "#import json\n",
    "#import pickle\n",
    "#import gzip\n",
    "#import copy\n",
    "#import array\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "#import numpy.lib.recfunctions as recfunc\n",
    "\n",
    "#from scipy.optimize import newton\n",
    "#from scipy.stats import norm\n",
    "\n",
    "import uproot\n",
    "\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "#from sklearn.tree import export_graphviz\n",
    "#from sklearn.inspection import permutation_importance\n",
    "#try:\n",
    "#    # See #1137: this allows compatibility for scikit-learn >= 0.24\n",
    "#    from sklearn.utils import safe_indexing\n",
    "#except ImportError:\n",
    "#    from sklearn.utils import _safe_indexing\n",
    "    \n",
    "#from livelossplot import PlotLossesKeras\n",
    "\n",
    "#from keras.losses import mean_squared_error, binary_crossentropy\n",
    "#from keras.models import Sequential, Model\n",
    "#from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "#from keras.layers import Lambda, Activation\n",
    "#from keras.optimizers import SGD\n",
    "#from tensorflow.keras.optimizers.legacy import Adam # for macos\n",
    "## from keras.optimizers import Adam # for non-macos\n",
    "#from keras.regularizers import l2\n",
    "#from tensorflow.keras.layers import BatchNormalization\n",
    "#from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "#import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data set\n",
    "\n",
    "We will use simulated events corresponding to three physics processes.\n",
    "\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan ($pp\\to Z/\\gamma^*$+jets) production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line ownloads the data only if you haven't done so yet\n",
    "\n",
    "if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "    !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n",
    "else:\n",
    "    print(\"Data were already donwloaded, I am not downloading them again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is made of three files, one per background (`background_1` is ttW, `background_2`is Drell-Yan.\n",
    "The ttH signal events file corresponds to only a percentage of the full data set: the rest is used in the data challenge (for schools where I offer a data challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "sig = uproot.open('data/signal_blind20.root')['Friends'].arrays(library=\"pd\")\n",
    "bk1 = uproot.open('data/background_1.root')['Friends'].arrays(library=\"pd\")\n",
    "bk2 = uproot.open('data/background_2.root')['Friends'].arrays(library=\"pd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection\n",
    "\n",
    "The first thing you need to do when building a machine learning model is to forget about the model, and **just look at the data**\n",
    "\n",
    "\n",
    "Let's start by looking at which features and labels are available in these files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variables are input features, corresponding to detector measurements of the properties of the reconstructed decay products.\n",
    "\n",
    "There are three special variables, though:\n",
    "\n",
    "- `Hreco_evt_tag`: this feature has values in ${0,1}$, where $1$ flags the event as signal event, and $0$ flags the event as background event;\n",
    "- `Hreco_HTXS_Higgs_pt`: this feature contains the true generate Higgs boson transverse momentum at generator level (used for regression);\n",
    "- `Hreco_HTXS_Higgs_y`: this feature contains the true generated Higgs boson rapidity (not pseudorapidity) at generator level (used for regression).\n",
    "\n",
    "You'll see down below that we will have of course to ignore some of all of these three features (they are not input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print a few lines from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histograms of some observables using matplotlib\n",
    "\n",
    "(see also examples on [matplotlib](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(sig[\"Hreco_Lep0_pt\"], bins=100)\n",
    "plt.xlabel(\"Lepton 1 $p_T$\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make scatter plots of one variable against the other: these are useful to assess the correlation between different features of the data.\n",
    "\n",
    "We'll see a bit below a way of doing so in an elegant way for all the features of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(sig[\"Hreco_Lep0_pt\"], sig[\"Hreco_Lep1_pt\"])\n",
    "plt.xlabel(\"Lepton0 pT\")\n",
    "plt.ylabel(\"Lepton1 pT\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot another variable, the transverse momentum of the third lepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(sig[\"Hreco_Lep2_pt\"], bins=100)\n",
    "plt.xlabel(\"Lepton0 pT\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on in the last plot?\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "The events preselected in these files contain events that have **at least two** leptons with same electrical charge. We therefore have in the data set a mix of events with two same-sign leptons and three leptons (out of which necessarily two will have the same sign).\n",
    "\n",
    "You have three choices:\n",
    "\n",
    "1. train our algorithm on `2lss` events, by selecting only events where the third lepton transverse momentum **is not set**: `dataframe=dataframe[dataframe['Hreco_Lep2_pt']==-99]`\n",
    "2. train our algorithm on `3l` events, by selecting only events where the third lepton transverse momentum **is set**: `dataframe=dataframe[dataframe['Hreco_Lep2_pt']>-99]`\n",
    "3. train our algorithm on `2lss+3l` events. The default value of $-99$ in this case will act as a semi-categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick option (1).\n",
    "\n",
    "We will first create a label for signal or background events (we could also use the `evt_tag` variable), then join all datasets together, then filter events to keep only those corresponding to 2lss events, and finally we will drop all the features corresponding to the third lepton, plus those corresponding to regression targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'label' and set its value to 1 or 0 for all rows (=events)\n",
    "sig['label'] = 1 \n",
    "bk1['label'] = 0\n",
    "bk2['label'] = 0\n",
    "\n",
    "# Merge the two backgrounds into one dataframe\n",
    "bkg = pd.concat([bk1, bk2])\n",
    "\n",
    "print(f\"bkg1 shape {bk1.shape}\")\n",
    "print(f\"bkg2 shape {bk2.shape}\")\n",
    "print(f\"bkg1+bkg2 shape {bkg.shape}\")\n",
    "\n",
    "# Merge the signal and background into one dataframe\n",
    "print(f\" Signal shape {sig.shape}\")\n",
    "print(f\" Bkg shape {bkg.shape}\")\n",
    "\n",
    "data = pd.concat([sig,bkg])\n",
    "\n",
    "print(f\" Data shape {data.shape}\")\n",
    "print(data.columns)\n",
    "\n",
    "# Filter data\n",
    "data=data[data['Hreco_Lep2_pt']==-99]\n",
    "# Drop unneeded features\n",
    "data = data.drop([\"index\",\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \n",
    "                  \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "\n",
    "\n",
    "print(f\" Data shape {data.shape}\")\n",
    "print(data.columns)\n",
    "print(f\"In this dataframe we finally have {data[data['label']==1].shape[0]} signal and {data[data['label']==0].shape[0]} background events\")\n",
    "print(\"Sanity check: look for NAN numbers in any of the rows or columns\")\n",
    "print(data.isna().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is still ordered, ie. all signal events are before the background events. Proper training requires a shuffled data set instead!\n",
    "\n",
    "We could also do that (and in fact we will) when randomly separating our dataset in training and test events, but it doesn't hurt to do it from the very beginning, to avoid forgetting it.\n",
    "\n",
    "We will also separate features and labels from each other, and check for corrupted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head(10)\n",
    "\n",
    "\n",
    "print(\"There are NaN-filled elements:\", data.isna().any().any())\n",
    "\n",
    "X = data.drop([\"label\"], axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "print(f\"data shape {data.shape}\")\n",
    "print(f\"input feature shape {X.shape}\")\n",
    "print(f\"label (=target) shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect all the pairwise correlations between features, for signal and background separately, as well as one-dimensional densities of individual features, by doing a `pairplot` using the useful library `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "cols_to_plot = [\n",
    "    'Hreco_Lep1_pt',\n",
    "    'Hreco_HadTop_pt',\n",
    "    'Hreco_All5_Jets_pt',\n",
    "    'Hreco_More5_Jets_pt',\n",
    "    'Hreco_Jets_plus_Lep_pt',\n",
    "    'label'\n",
    "]\n",
    "pp=sns.pairplot(data=data.sample(1000)[cols_to_plot], hue='label', diag_kws={'bw_method': 0.2}) # Scatter plot\n",
    "pp.map_lower(sns.kdeplot, levels=4, color=\".2\") # Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "- what happens if you plot the full list of variables (`cols_to_plot = data.columns`) from the command above?\n",
    "- what happens if you omit `.sample(100)` from the command above?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set into training and test set\n",
    "\n",
    "When we train a machine learning algorithm, we are trying to solve an interpolation problem (*find the function of the input features that provides the best approximation of the true function*) by also requiring that the solution generalizes sufficiently well (*the interpolating function must also predict correctly the value of the true function for new, unseen data*).\n",
    "\n",
    "\n",
    "When we have a labelled dataset, we will therefore split it into: a *training set*, which we will use to train the machine learning algorithm; a *test set*, which we will use to evaluate the performance of the algorithm for various realizations of the algorithm (e.g. tuning hyperparameters); and an *application set*, which are the data we are really interested in studying in the end.\n",
    "\n",
    "For many applications, when the amount of hyperparameters tuning is moderate, application set and test set can be collapsed into a single set (usually called *test set*). This is what we will do in this tutorial.\n",
    "\n",
    "![Blah](figs/trainingNetwork.png)\n",
    "\n",
    "(Image: P. Vischia, [doi:10.5281/zenodo.6373442](https://doi.org/10.5281/zenodo.6373442))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"We have {len(X_train_orig)} training samples with {sum(y_train_orig)} signal and {sum(1-y_train_orig)} background events\")\n",
    "print(f\"We have {len(X_test_orig)} testing samples with {sum(y_test_orig)} signal and {sum(1-y_test_orig)} background events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrices\n",
    "\n",
    "For classification problems, another important thing is to take a look at the correlations between all the variables, in events belonging to each class separately.\n",
    "\n",
    "Looking at the correlation between features can highlight pairs that are strongly correlated with each other, and one could decide to omit them since they do not add further information when the correlations are very high.\n",
    "\n",
    "We look at the correlation for each class because we are very interested in pairs of features that have different correlation in one class or the other (in our example, signal or background).\n",
    "\n",
    "Now we will choose a simple criterion, for instance the value of one of the features that characterize the houses, and use it to decide if the house is in New York (we want to predict 0) or in San Francisco (we want to predict 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrmatrix(corr, label):\n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1., vmax=1., center=0.,\n",
    "        cmap=sns.diverging_palette(20., 220., n=200, as_cmap=True),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    );\n",
    "\n",
    "    ax.set_title('Correlation matrix for %s events' % label)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "corrmatrix(X_train_orig[y_train_orig==1.].corr(), 'signal')\n",
    "corrmatrix(X_train_orig[y_train_orig==0.].corr(), 'background')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have plotted is the Pearson correlation coefficient, which leads to an important limitation of this diagnostic tool.\n",
    "\n",
    "The Pearson correlation coefficient captures only **linear** correlation between variables, and is blind to many nonlinear correlations that there may be. Don't trust the above matrices blindly.\n",
    "\n",
    "\n",
    "![Figure from BDN2010](figs/corrcov.png)\n",
    "\n",
    "(figure from C. Delaere slides at the 2010 BND school)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "Before training a first ML-based classifier we need to think about if any preprocessing of the data is required. Many ML algorithms are based on gradient minimization techniques that can fail if the inputs have numbers that widely-vary in magnitude. For example, the $p_T$ of a jet might range from 20 to 2000 GeV, covering several orders of magnitude, and can prevent the convergence of a minimization algorithm. In the case of $p_T$ a typical manual preprocessing could be to use the logarithm as input instead, ie. $\\log(p_T/1~\\textrm{GeV})$, which in particular moves.\n",
    "\n",
    "In this data set, the features have already been scaled such that their range is around unity. Sometimes this happens naturally, but in this case several variables come directly from particle physics and represent momenta of particles produced in high-energy interaction: when expressed in GeV, these variables will most certainly **not** be in a range close to unity.\n",
    "\n",
    "Common choices to preprocess input features automatically are minmax scaling or normalization.\n",
    "\n",
    "#### Minmax\\n\",\n",
    "Compress the range linearly:\n",
    "$$X_{scaled} = \\\\\\\\frac{X-X_{min}}{X_{max}-X_{min}}$$\n",
    "A drawback is that this results in an artificially smaller variance (the range is compressed linearly), which can deform the effect of outliers.\n",
    "\n",
    "#### Standardization\n",
    "Compress the range and the shape:\n",
    "$$X_{normalized} = \\\\\\\\frac{X - \\\\\\\\mu}{\\\\\\\\sigma}$$\n",
    "where $\\\\\\\\mu$ is the mean of the feature values and $\\\\\\\\sigma$ is the standard deviation.\n",
    "\n",
    "#### Which one?\\n\",\n",
    "- Typically one would use minmax scaling when your features are remarkably nongaussian and your ML algorithm of choice doesn't require Gaussian inputs. The price is that it affects outliers.\n",
    "- Typically one would use normalization when the features are approximately Gaussian or when your ML algorithm of choice requires Gaussian inputs. However, it also results in numbers close to 1 (minimization algorithms and gradient descend love numbers that are not too large or too small), so it can be used for any algorithm: the good news is that it doesn't affect outliers.\n",
    "\n",
    "You can also apply PCA, as we have discussed in the lectures. The code below imports the most common scalers, for you to play with. For source identification e.g. in the processing of periodic signals (sound waves, radio signals, etc), you can also use ICA (Indipendent Component Analysis), which is rather powerful.\n",
    "\n",
    "For now, let's use the standard scaler for the input features of the dense NN. You are encouraged to try out the others!\n",
    "\n",
    "We will also store the original train and test structures, because for the convolutional network we will preprocess them differently.\n",
    "\n",
    "We will also store the original train and test structures, so that you can try out various preprocessing techniques and play at the same time with many of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv = X_train_orig.copy()\n",
    "y_train_conv = y_train_orig.copy()\n",
    "X_test_conv = X_test_orig.copy()\n",
    "y_test_conv = y_test_orig.copy()\n",
    "\n",
    "X_train = X_train_orig.copy()\n",
    "y_train = y_train_orig.copy()\n",
    "X_test = X_test_orig.copy()\n",
    "y_test = y_test_orig.copy()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler, # maxAbs\n",
    "    MinMaxScaler, # MinMax\n",
    "    Normalizer, # Normalization (equal integral)\n",
    "    StandardScaler# standard scaling\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the input features and the target variable\n",
    "for column in X_train.columns:\n",
    "    scaler = StandardScaler().fit(X_train.filter([column], axis=1))\n",
    "    X_train[column] = scaler.transform(X_train.filter([column], axis=1))\n",
    "    X_test[column] = scaler.transform(X_test.filter([column], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use the basic syntax recommended by the documentation, as follows, but then you would be standardizing all the features to exacly the same mean. This may work for some data sets, but for this specific one it does not (it actually significantly worsens the performance---you can try ;) ).\n",
    "\n",
    "```\n",
    "scaler = StandardScaler().fit(X_train[X_train.columns])\n",
    "X_train[X_train.columns] = scaler.transform(X_train[X_train.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple tree-based classifier\n",
    "\n",
    "The act of selecting regions of a data set by \"cutting\" (imposing thresholds) on some of the features is very natural for the particle physicist, so let's start by training a tree-based classifier.\n",
    "\n",
    "Decision trees are precisely that:\n",
    "\n",
    "<img src=\"figs/bdt_en_edit.png\" alt=\"bdtexample\" style=\"width:80%;\"/>\n",
    "\n",
    "(image from [r2d3.us](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/))\n",
    "\n",
    "Decision trees depend however on the random order of imposing cuts on the data set. To reduce the dependence on the starting point and the ordering of the selection process, we will use an *ensemble* of decision trees, which will cut at random the data set, and we will *pool* the classification answers from all the trees via e.g. a majority vote.\n",
    "\n",
    "Not all the trees resulting from the random cuts will make sense, so we will use a procedure called *boosting*, which consists in weighting each random tree based on its own performance. The weights will be used to decide how to generate the next trees.\n",
    "\n",
    "\n",
    "<img src=\"figs/boosting.png\" alt=\"boosting\" style=\"width:80%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "bdt_ada_orig = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3, criterion='log_loss'), n_estimators=100, learning_rate=learning_rate, random_state=42)\n",
    "bdt_grad_orig = GradientBoostingClassifier(n_estimators=100, learning_rate=learning_rate, max_depth=3, random_state=42, verbose=1)\n",
    "\n",
    "bdt_ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3, criterion='log_loss'), n_estimators=100, learning_rate=learning_rate, random_state=42)\n",
    "bdt_grad = GradientBoostingClassifier(n_estimators=100, learning_rate=learning_rate, max_depth=3, random_state=42, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it takes too much to run, try downsampling:\n",
    "Ntrain=10000\n",
    "Ntest=2000\n",
    "X_train = X_train[:Ntrain]\n",
    "y_train = y_train[:Ntrain]\n",
    "X_test = X_test[:Ntest]\n",
    "y_test = y_test[:Ntest]\n",
    "\n",
    "X_train_orig = X_train_orig[:Ntrain]\n",
    "X_test_orig = X_test_orig[:Ntest]\n",
    "\n",
    "print(\"Training with adaptive boost\")\n",
    "fitted_bdt_ada=bdt_ada.fit(X_train, y_train)\n",
    "fitted_bdt_ada_orig=bdt_ada_orig.fit(X_train_orig, y_train)\n",
    "\n",
    "print(\"Training with gradient boost\")\n",
    "fitted_bdt_grad=bdt_grad.fit(X_train, y_train)\n",
    "fitted_bdt_grad_orig=bdt_grad_orig.fit(X_train_orig, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the performance of the classifier\n",
    "\n",
    "A simple performance estimate for the classifier is the mean accuracy on a certain data set.\n",
    "Let's print it out for the training set and for the test set.\n",
    "\n",
    "Let's also print some metric as a function of the training iteration. Since this is a classification problem, we will use the accuracy on the test sample.\n",
    "\n",
    "Accuracy is defined as $\\frac{\\text{correct predictions}}{\\text{all predictions}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adaptive boost: train accuracy', fitted_bdt_ada.score(X_train, y_train),', test accuracy', fitted_bdt_ada.score(X_test, y_test))\n",
    "print('Gradient boost: train accuracy', fitted_bdt_grad.score(X_train, y_train),', test accuracy', fitted_bdt_grad.score(X_test, y_test))\n",
    "\n",
    "print('Adaptive boost (non-scaled data): train accuracy', fitted_bdt_ada_orig.score(X_train_orig, y_train),', test accuracy', fitted_bdt_ada_orig.score(X_test_orig, y_test))\n",
    "print('Gradient boost (non-scaled data): train accuracy', fitted_bdt_grad_orig.score(X_train_orig, y_train),', test accuracy', fitted_bdt_grad_orig.score(X_test_orig, y_test))\n",
    "\n",
    "\n",
    "def get_loss_vs_iteration(x_features, y_labels, classifier):\n",
    "    return np.array(list(map(\n",
    "        lambda score: sklearn.metrics.log_loss(y_labels,score), classifier.staged_predict_proba(x_features)\n",
    "    )))\n",
    "\n",
    "train_loss_bdt_ada = get_loss_vs_iteration(X_train, y_train, fitted_bdt_ada)\n",
    "test_loss_bdt_ada = get_loss_vs_iteration(X_test, y_test, fitted_bdt_ada)\n",
    "\n",
    "\n",
    "train_loss_bdt_grad = get_loss_vs_iteration(X_train, y_train, fitted_bdt_grad)\n",
    "test_loss_bdt_grad = get_loss_vs_iteration(X_test, y_test, fitted_bdt_grad)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(train_loss_bdt_ada)),train_loss_bdt_ada,label=\"Ada boost (train)\",color='royalblue',linestyle='-')\n",
    "plt.plot(np.arange(len(test_loss_bdt_ada)),test_loss_bdt_ada,label=\"Ada boost (test)\",color='royalblue',linestyle='--')\n",
    "\n",
    "plt.plot(np.arange(len(train_loss_bdt_grad)),train_loss_bdt_grad,label=\"Gradient boost (train)\",color='orange',linestyle='-')\n",
    "plt.plot(np.arange(len(test_loss_bdt_grad)),test_loss_bdt_grad,label=\"Gradient boost (test)\",color='orange',linestyle='--')\n",
    "         \n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe two things:\n",
    "\n",
    "The performance of gradient-based boosting is better than of adaptive boosting, which is normally expected.\n",
    "\n",
    "For adaptive boosting, after about 20 iterations the performance on both the training and test data sets starts degrading: it's clear that the training should stop at about 20 iterations.\n",
    "\n",
    "For gradient boosting, the performance on the train set begins to be remarkably lower than that on the training set after a few iteration, and the performance on the training set keeps improving. All of this is an indication that our algorithm is able to separate the two classes (signal and background) very effectively, but generalizes somehow poorly to data not seen during the training. Remember the definition of overtraining given during the lectures.\n",
    "\n",
    "To study the generalization properties of machine learning algorithms we will switch to neural networks, where we can control in an intuitive way the complexity of the network.\n",
    "\n",
    "For the moment, however, let's look at some additional ways of exploring the algorithm's performance.\n",
    "\n",
    "Exercise:\n",
    "- What happens if you change the learning rate to `1.0`? What happens if you change it to `0.01`? Do you need to tweak the amount of iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, though, let's compare the training of our scaled vs non-scaled classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss_bdt_ada_orig = get_loss_vs_iteration(X_train_orig, y_train, fitted_bdt_ada_orig)\n",
    "test_loss_bdt_ada_orig = get_loss_vs_iteration(X_test_orig, y_test, fitted_bdt_ada_orig)\n",
    "\n",
    "\n",
    "train_loss_bdt_grad_orig = get_loss_vs_iteration(X_train_orig, y_train, fitted_bdt_grad_orig)\n",
    "test_loss_bdt_grad_orig = get_loss_vs_iteration(X_test_orig, y_test, fitted_bdt_grad_orig)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(train_loss_bdt_ada)),train_loss_bdt_ada-train_loss_bdt_ada_orig,label=\"Ada boost (train)\",color='royalblue',linestyle='-')\n",
    "plt.plot(np.arange(len(test_loss_bdt_ada)),test_loss_bdt_ada-test_loss_bdt_ada_orig,label=\"Ada boost (test)\",color='royalblue',linestyle='--')\n",
    "\n",
    "plt.plot(np.arange(len(train_loss_bdt_grad)),train_loss_bdt_grad-train_loss_bdt_grad_orig,label=\"Gradient boost (train)\",color='orange',linestyle='-')\n",
    "plt.plot(np.arange(len(test_loss_bdt_grad)),test_loss_bdt_grad-test_loss_bdt_grad_orig,label=\"Gradient boost (test)\",color='orange',linestyle='--')\n",
    "         \n",
    "plt.ylabel(\"loss_scaled-loss_unscaled\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is going on???\n",
    "\n",
    "Boosted decision trees are an ensemble method that consists in building many decision trees, i.e. classifiers based on thresholds (\"cuts\", in particle physics) on the features.\n",
    "\n",
    "Scalers modifies the values of each feature but **they preserve the ordering**, and therefore scaling or not scaling doesn't change in any way the results of the algorithm.\n",
    "\n",
    "You will see in the next tutorial that scaling is, on the contrary, very important for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The classifier output\n",
    "\n",
    "Let's take a look at the resulting classfiers, for the two classes and the two algorithms. This is always done exclusively on the test data set.\n",
    "\n",
    "The `decision_function(input, k)` gives a score that, for binary classification ($k==1$) is closer to -1 if the event is classified as background-like and closer to +1 if the event is classified as signal-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(fitted_bdt_ada.decision_function(X_test[y_test==0]), bins=20, alpha=0.7, label=\"Score for background events\")\n",
    "plt.hist(fitted_bdt_ada.decision_function(X_test[y_test==1]), bins=20, alpha=0.7, label=\"Score for signal events\")\n",
    "plt.title(\"Adaptive Boost\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(fitted_bdt_grad.decision_function(X_test[y_test==0]), bins=20, alpha=0.7, label=\"Score for background events\")\n",
    "plt.hist(fitted_bdt_grad.decision_function(X_test[y_test==1]), bins=20, alpha=0.7, label=\"Score for signal events\")\n",
    "plt.title(\"Gradient Boost\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret these distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve\n",
    "\n",
    "As we have seen in the lecture, the Receiver Operating Characteristic (ROC) curve is a handy performance plot for the training of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rocs(scores_and_names, y):\n",
    "    pack=[] \n",
    "    for s, n in scores_and_names: \n",
    "        fpr, tpr, thresholds = roc_curve(y.ravel(), s)\n",
    "        pack.append([n, fpr,tpr,thresholds])\n",
    "\n",
    "    plt.figure()\n",
    "    lw=2\n",
    "    for n, fpr, tpr, thresholds in pack:\n",
    "        plt.plot(fpr, tpr, lw=lw, label=\"%s (AUC = %0.2f)\" % (n, auc(fpr, tpr))) \n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the ROC curve for our two classifiers. To do so, we have first to convert the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocs([ [fitted_bdt_ada.decision_function(X_test), 'AdaBoost'],\n",
    "            [fitted_bdt_grad.decision_function(X_test), 'GradBoost']],\n",
    "          y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve only reports the TPR and FRP for all possible thresholds of the classifier.\n",
    "Once we decide a threshold (conventionally, 0.5), we can also plot the confusion matrix for that choice.\n",
    "\n",
    "In the case of ensemble classifiers such as boosted decision trees, `predict` gives the predicted class of an input sample as the weighted mean prediction of the classifiers in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_plot_confusion_matrix(clf, X, y, label=\"\"):\n",
    "    cm = confusion_matrix(y, clf.predict(X), labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    plt.figure()\n",
    "    disp.plot()\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "print_and_plot_confusion_matrix(fitted_bdt_ada, X_test, y_test, \"Adaptive Boost\")\n",
    "print_and_plot_confusion_matrix(fitted_bdt_grad, X_test, y_test, \"Gradient Boost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers are pretty bad, right?\n",
    "\n",
    "Exercise:\n",
    "- can you find ways of improving them? In the next exercises we will train a neural network to solve the same problem, but for now you can try:\n",
    "    - tweak parameters of the classifiers (learning rate, iterations, etc)\n",
    "    - tweak the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect in detail the model structure and its correlation with a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "plt.figure(figsize=[25,18])\n",
    "sklearn.tree.plot_tree(fitted_bdt_grad.estimators_[42, 0],feature_names=data.columns, fontsize=14)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Alternative method:# Pick one of the trees (maybe modify to pick the tree with largest weight or something like that)\n",
    "#sub_tree_42 = fitted_bdt_grad.estimators_[42, 0]\n",
    "## Visualization\n",
    "## Install graphviz: https://www.graphviz.org/download/\n",
    "#from pydotplus import graph_from_dot_data\n",
    "#from IPython.display import Image\n",
    "#dot_data = export_graphviz(\n",
    "#    sub_tree_42,\n",
    "#    out_file=None, filled=True, rounded=True,\n",
    "#    special_characters=True,\n",
    "#    proportion=False, impurity=False, # enable them if you want\n",
    "#)\n",
    "#graph = graph_from_dot_data(dot_data)\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "A good question to pose yourself is whether the features you have chosen for your data are meaningful variables (i.e. if they are actually relevant to your classifier). Another good question is which features drive the prediction for a given event or set of events.\n",
    "\n",
    "All these questions can be answered by using different concepts:\n",
    "\n",
    "- **Permutation importance**: the decrease in a model score when a single feature value is randomly shuffled (scikit-learn docs) (akin to impacts for profile likelihood fits)\n",
    "Shapley values: based on game theory (see other contribution)\n",
    "Correlation-based: e.g. parallel coordinates in TMVA: look where each variable is mapped\n",
    "to/correlated with\n",
    "\n",
    "- **Perturbational approach**: perturbing the value of a feature and looking at the change of the prediction gives hints on how important the variable is for the method. This is at the basis of LIME (`pip install lime`). You can read more [here](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)\n",
    "\n",
    "- **Game-theoretical approach**: consider the prediction task as a game in game theory, and the features as players who bet via their values. The payout, as difference of prediction with respect to the true value, estimates how much a feature pushes the prediction away from the truth.\n",
    "\n",
    "- **Visual approach**: parallel coordinates, which were implemented in ROOT TMVA, let you handily select a range in the prediction, and have a visual representation of which ranges of each feature is mapped into that region of the prediction.\n",
    "\n",
    "![Parallel coordinates (reference in the figure)](figs/parcoord.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Permutation importance\n",
    "\n",
    "The idea is: randomly shuffle one single feature value, then check how much does the prediction change. If the prediction decreases by a lot, then the value of the feature is crucial to the prediction.\n",
    "\n",
    "Note: the importance is always **relative to a specific model**, it has no absolute validity. A feature that is deemed low-importance for a badly designed model may be deemed high-importance for a good model, and viceversa. Permutation importance scores don't \"talk\" across different models.\n",
    "\n",
    "Also, if the model has a performance which is near-chance, then it is not strongly predictive, so the answers one may get from permutation importance scores are not really reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\n",
    "\n",
    "rs = permutation_importance(\n",
    "    fitted_bdt_grad, X_test, y_test, n_repeats=30, random_state=0, scoring=scoring)\n",
    "\n",
    "for metric in rs:\n",
    "    print(f\"{metric}\")\n",
    "    r = rs[metric]\n",
    "    for i in r.importances_mean.argsort()[::-1]:\n",
    "        if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "            print(f\"    {X_train.columns[i]:<8}\" # +1 to skip the label in the naming\n",
    "                  f\"{r.importances_mean[i]:.3f}\"\n",
    "                  f\" +/- {r.importances_std[i]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shapley values\n",
    "(based on [this blog post](https://www.analyticsvidhya.com/blog/2019/11/shapley-value-machine-learning-interpretability-game-theory/))\n",
    "\n",
    "Shapley values are a construct based on game theory.\n",
    "The main idea behind Shapley values is to consider the prediction task for a single event as game played by the feature values of that event. The features collaborate together to play the game by betting. The value of the feature is the amount each feature bets on the prediction task. The **Shapley Value** for each feature is the payout of the game, and consists in the correct weight such that the sum of all Shapley values for the features is the difference between the predictions and the average value of the model. In other words, the Shapley value represents how much each variable pushes the prediction far from the expected value.\n",
    "\n",
    "More concretely, the Shapley value for a feature A is computed as follows:\n",
    "\n",
    "- Get all subsets of features that do not contain A\n",
    "- Compute the effect of adding A to each of these subsets \n",
    "- Aggregate all the contributions (i.e. compute the marginal contribution of the feature over all the subsets)\n",
    "\n",
    "In principle we should retrain the model for each of these subsets, but instead we (the `shap` package, actually) will just compute predictions by replacing the value of the feature with its own average value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(fitted_bdt_grad)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "i = 500\n",
    "shap.force_plot(explainer.expected_value, shap_values[i], features=X_train.iloc[i], feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values in blue represent features that push the prediction towards negative values, values in red represent features that push the prediction towards positive values, *for the event number 4776*.\n",
    "\n",
    "We naturally want a summary of Shapley values over all observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns, use_log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides looking at the more important variables, you may also look at the less important, to **prune** them.\n",
    "\n",
    "Pruning consists in dropping the least important variables and retraining your machine learning algorithm.\n",
    "The idea behind it is that the variables dropped don't influence the prediction anyway, and retraining without them should give more or less the same performance but with a simpler model. Why would we do that? Well, for example inference may be time-sensitive, and simpler models are computationally **faster** to evaluate.\n",
    "\n",
    "Exercise:\n",
    "- Prune the least important variables and train new classifiers with less variables. Compare the performance, and time the execution using the python library `time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end (for now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
