{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0952e0c-fcba-4cdf-9e19-3dcf6d141793",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/pv_data_science_school/blob/master/2b_supervised_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee132c84-4b81-443c-a4d2-b043fd8b051a",
   "metadata": {},
   "source": [
    "# Machine Learning School, ICNFP 2025 edition\n",
    "## Exercise 2b: headaches in training neural networks: classification\n",
    "\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f957cb-de66-43cc-9ffe-20c1f7375d88",
   "metadata": {},
   "source": [
    "## Setup the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c41b5e-8e15-4862-8294-1ade06c03c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "runOnColab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d76217e-74ac-4955-a5eb-7b44b596468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runOnColab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd \"/content/drive/MyDrive/\"\n",
    "    if not os.path.isdir(\"pv_data_science_school\"): \n",
    "        %git clone https://github.com/vischia/pv_data_science_school.git\n",
    "    %cd pv_data_science_school\n",
    "#!pwd\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206bc2c9-8337-4ad9-a169-f17a5957dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version 2.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "import uproot\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "print('Using torch version', torch.__version__)\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True) #Usually overkill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d1ea1-94c2-475e-97a5-4f4f0d344f93",
   "metadata": {},
   "source": [
    "We will use simulated events corresponding to three physics processes.\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan ($pp\\\\to Z/\\\\gamma^*$+jets) production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%\"/>\n",
    "\n",
    "We use the [uproot](https://uproot.readthedocs.io/en/latest/basic.html) library to conveniently read in a [ROOT TNuple](https://root.cern.ch/doc/master/classTNtuple.html) which can automatically convert it to a [pandas dataframe](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227bbfc4-437b-4466-bc46-3788d1150d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data were already downloaded, I am not downloading them again.\n"
     ]
    }
   ],
   "source": [
    "# This line downloads the data only if you haven't done so yet\n",
    "\n",
    "if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "    !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n",
    "else:\n",
    "    print(\"Data were already downloaded, I am not downloading them again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148bec0d-a2cb-477f-9822-2c921c7e3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "sig = uproot.open('data/signal_blind20.root')['Friends'].arrays(library=\"pd\")\n",
    "bk1 = uproot.open('data/background_1.root')['Friends'].arrays(library=\"pd\")\n",
    "bk2 = uproot.open('data/background_2.root')['Friends'].arrays(library=\"pd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02abe17-37fe-4ac9-9518-4fd8237b4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bkg1 shape (149644, 35)\n",
      "bkg2 shape (149644, 35)\n",
      "bkg1+bkg2 shape (299288, 35)\n",
      " Signal shape (299287, 36)\n",
      " Bkg shape (299288, 35)\n",
      " Data shape (598575, 36)\n",
      "Index(['index', 'Hreco_Lep0_pt', 'Hreco_Lep1_pt', 'Hreco_Lep2_pt',\n",
      "       'Hreco_HadTop_pt', 'Hreco_All5_Jets_pt', 'Hreco_More5_Jets_pt',\n",
      "       'Hreco_Jets_plus_Lep_pt', 'Hreco_Lep0_eta', 'Hreco_Lep1_eta',\n",
      "       'Hreco_Lep2_eta', 'Hreco_HadTop_eta', 'Hreco_All5_Jets_eta',\n",
      "       'Hreco_More5_Jets_eta', 'Hreco_Jets_plus_Lep_eta', 'Hreco_Lep0_phi',\n",
      "       'Hreco_Lep1_phi', 'Hreco_Lep2_phi', 'Hreco_HadTop_phi',\n",
      "       'Hreco_All5_Jets_phi', 'Hreco_More5_Jets_phi',\n",
      "       'Hreco_Jets_plus_Lep_phi', 'Hreco_Lep0_mass', 'Hreco_Lep1_mass',\n",
      "       'Hreco_Lep2_mass', 'Hreco_HadTop_mass', 'Hreco_All5_Jets_mass',\n",
      "       'Hreco_More5_Jets_mass', 'Hreco_Jets_plus_Lep_mass', 'Hreco_TopScore',\n",
      "       'Hreco_met', 'Hreco_met_phi', 'Hreco_HTXS_Higgs_pt',\n",
      "       'Hreco_HTXS_Higgs_y', 'Hreco_evt_tag', 'label'],\n",
      "      dtype='object')\n",
      "data shape (392743, 24)\n",
      "input feature shape (392743, 23)\n",
      "label (=target) shape (392743,)\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'label' and set its value to 1 or 0 for all rows (=events)\n",
    "sig['label'] = 1 \n",
    "bk1['label'] = 0\n",
    "bk2['label'] = 0\n",
    "\n",
    "bk1=bk1.sample(frac=sig.shape[0]/bk1.shape[0]/2).reset_index(drop=True)\n",
    "bk2=bk2.sample(frac=sig.shape[0]/bk2.shape[0]/2).reset_index(drop=True)\n",
    "# Merge the two backgrounds into one dataframe\n",
    "bkg = pd.concat([bk1, bk2])\n",
    "\n",
    "print(f\"bkg1 shape {bk1.shape}\")\n",
    "print(f\"bkg2 shape {bk2.shape}\")\n",
    "print(f\"bkg1+bkg2 shape {bkg.shape}\")\n",
    "\n",
    "# Merge the signal and background into one dataframe\n",
    "print(f\" Signal shape {sig.shape}\")\n",
    "print(f\" Bkg shape {bkg.shape}\")\n",
    "\n",
    "data = pd.concat([sig,bkg])\n",
    "\n",
    "print(f\" Data shape {data.shape}\")\n",
    "print(data.columns)\n",
    "\n",
    "# Filter data\n",
    "data=data[data['Hreco_Lep2_pt']==-99]\n",
    "# Drop unneeded features\n",
    "data = data.drop([\"index\",\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \n",
    "                  \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\", \"Hreco_More5_Jets_pt\", \"Hreco_More5_Jets_eta\", \"Hreco_More5_Jets_phi\", \"Hreco_More5_Jets_mass\",], axis=1 )\n",
    "\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = data.drop([\"label\"], axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "print(f\"data shape {data.shape}\")\n",
    "print(f\"input feature shape {X.shape}\")\n",
    "print(f\"label (=target) shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563813ad-37b4-435c-9aa2-42ac9f47375c",
   "metadata": {},
   "source": [
    "## Train a dense neural network\n",
    "\n",
    "\n",
    "For neural networks we will use `pytorch`, a backend designed natively for tensor operations.\n",
    "I prefer it to tensorflow, because it exposes (i.e. you have to call them explicitly in your code) the optimizer steps and the backpropagation steps.\n",
    "\n",
    "You could also use the `tensorflow` backend, either directly or through the `keras` frontend.\n",
    "Saying \"I use keras\" does not tell you which backend is being used. It used to be either `tensorflow` or `theano`. Nowadays `keras` is I think almost embedded inside tensorflow, but it is still good to specify.\n",
    "\n",
    "`torch` handles the data management via the `Dataset` and `DataLoader` classes.\n",
    "Here we don't need any specific `Dataset` class, because we are not doing sophisticated things, but you may need that in the future.\n",
    "\n",
    "The `Dataloader` class takes care of providing quick access to the data by sampling batches that are then fed to the network for (mini)batch gradient descent.\n",
    "\n",
    "We'll also calculate the proportion of signal to background events in the data sample, to rescale the class weights appropriately\n",
    "                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a0572-a6ab-4c47-bd63-482733352451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"We have\", len(X_train), \"training samples and \", len(X_test), \"testing samples\")\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.Tensor(X.values if isinstance(X, pd.core.frame.DataFrame) else X).to(device)\n",
    "        self.y = torch.Tensor(y.values).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        datum = self.X[idx]\n",
    "        \n",
    "        return datum, label\n",
    "\n",
    "batch_size=512 # Minibatch learning\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "# Unscaled features\n",
    "train_dataset_orig = MyDataset(X_train_orig, y_train_orig)\n",
    "test_dataset_orig = MyDataset(X_test_orig, y_test_orig)\n",
    "\n",
    "train_dataloader_orig = DataLoader(train_dataset_orig, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_dataloader_orig = DataLoader(test_dataset_orig, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "# Scaled features\n",
    "train_dataset_scaled = MyDataset(X_train_scaled, y_train_scaled)\n",
    "test_dataset_scaled = MyDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "train_dataloader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_dataloader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader_scaled))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "# Assume y_train is a 1D NumPy array or pandas Series with 0/1 labels\n",
    "n_pos = np.sum(y_train_orig == 1)\n",
    "n_neg = np.sum(y_train_orig == 0)\n",
    "\n",
    "# Calculate positive class weight (negatives / positives)\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32)\n",
    "print(pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9422ec-c810-452b-82f5-edff81dcf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.Tensor(X.values if isinstance(X, pd.core.frame.DataFrame) else X).to(device)\n",
    "        self.y = torch.Tensor(y.values).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        datum = self.X[idx]\n",
    "        \n",
    "        return datum, label\n",
    "\n",
    "batch_size=512 # Minibatch learning\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "# Unscaled features\n",
    "train_dataset_orig = MyDataset(X_train_orig, y_train_orig)\n",
    "test_dataset_orig = MyDataset(X_test_orig, y_test_orig)\n",
    "\n",
    "train_dataloader_orig = DataLoader(train_dataset_orig, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_dataloader_orig = DataLoader(test_dataset_orig, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "# Scaled features\n",
    "train_dataset_scaled = MyDataset(X_train_scaled, y_train_scaled)\n",
    "test_dataset_scaled = MyDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "train_dataloader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_dataloader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader_scaled))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "# Assume y_train is a 1D NumPy array or pandas Series with 0/1 labels\n",
    "n_pos = np.sum(y_train_orig == 1)\n",
    "n_neg = np.sum(y_train_orig == 0)\n",
    "\n",
    "# Calculate positive class weight (negatives / positives)\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32)\n",
    "print(pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe9146-6213-40c9-a91d-f78341e1e826",
   "metadata": {},
   "source": [
    "Let's build a simple neural network, by inheriting from the `nn.Module` class. **This is very crucial, because that class is the responsible for providing the automatic differentiation infrastructure for tracking parameters and performing backpropagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a1cac-2fe6-4d02-b899-b4ba8b707161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ninputs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid() # No sigmoid if using BCEWithLogitsLoss\n",
    "        )\n",
    "        self.linear_relu_stack.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ee743-cc12-4a5e-8e36-65f809b634a6",
   "metadata": {},
   "source": [
    "Let's instantiate the neural network and print some info about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef615813-e4c5-403e-8488-7008cdce36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train_orig.shape[1])\n",
    "\n",
    "print(model) # some basic info\n",
    "\n",
    "print(\"Now let's see some more detailed info by using the torchinfo package\")\n",
    "torchinfo.summary(model, input_size=(batch_size, X_train_orig.shape[1])) # the input size is (batch size, number of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac26317-34cd-4dc0-870d-7080b4a860cf",
   "metadata": {},
   "source": [
    "Now let's introduce a crucial concept: `torch` lets you manage in which device you want to put your data and models, to optimize access at different stages.\n",
    "\n",
    "Let's do that by, for educational purposes, accessing the data loader via its iterator, and sample a single batch by calling `next` on the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cf99c-1c0a-4679-ba59-c7a3b3f9a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available() and torch.cuda.device_count()>0:\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "print (\"Available device: \",device)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "random_batch_X, random_batch_y = next(iter(train_dataloader_orig))\n",
    "\n",
    "print(\"The original dataloader resides in\", random_batch_X.get_device())\n",
    "\n",
    "# Let's reinstantiate the dataset\n",
    "\n",
    "# Unscaled features\n",
    "train_dataset_orig = MyDataset(X_train_orig, y_train_orig, device=device)\n",
    "test_dataset_orig = MyDataset(X_test_orig, y_test_orig, device=device)\n",
    "\n",
    "train_dataloader_orig = DataLoader(train_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_orig = DataLoader(test_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Scaled features\n",
    "train_dataset_scaled = MyDataset(X_train_scaled, y_train_scaled, device=device)\n",
    "test_dataset_scaled = MyDataset(X_test_scaled, y_test_scaled, device=device)\n",
    "\n",
    "train_dataloader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "random_batch_X, random_batch_y = next(iter(train_dataloader_orig))\n",
    "\n",
    "print(\"The new dataloader puts the batches in in\", random_batch_X.get_device())\n",
    "\n",
    "# Reinstantiate the model, on the chosen device\n",
    "model = NeuralNetwork(X_train_orig.shape[1], device)\n",
    "\n",
    "#check if the NN can be evaluated some data; note: it has not been trained yet\n",
    "print (model(torch.tensor(X_train_orig.values[:10],device=device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe94c7f-186b-47fb-a7e3-65ce99661837",
   "metadata": {},
   "source": [
    "We have learned how load the data into the GPU, how to define and instantiate a model. Now we need to define a training loop.\n",
    "\n",
    "In `keras`, this is wrapped hidden into the `.fit()` method, which I think is bad because it hides the actual procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60e984-c72d-42a3-b04d-73a6b2204af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device, disable=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    #for batch, (X, y) in enumerate(dataloader):\n",
    "    best_loss = np.inf\n",
    "    for (X,y) in tqdm(dataloader, disable=disable):\n",
    "        # Reset gradients (to avoid their accumulation)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #if (all_equal3(pred.detach().numpy())):\n",
    "        #    print(\"All equal!\")\n",
    "        loss = loss_fn(pred.squeeze(dim=1), y)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss.detach().cpu()\n",
    "            torch.save(model.state_dict(), best_model_path) # Save the full state of the model, to have access to the training history\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e03ac-147b-4895-85c0-1a77d5cbe2df",
   "metadata": {},
   "source": [
    "Now we need to define the loop that is run on the test dataset.\n",
    "\n",
    "**The test dataset is just used for evaluating the output of the model. No backpropagation is needed, therefore backpropagation must be switched off!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26311c24-455f-41b7-a5b6-0563071dad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, device, disable=False):\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        #for X, y in dataloader:\n",
    "        for (X,y) in tqdm(dataloader, disable=disable):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred.squeeze(dim=1), y).item()\n",
    "            losses.append(loss)\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c0895-2ffd-4543-aa29-f40740de3241",
   "metadata": {},
   "source": [
    "We are now read to train this network!\n",
    "At the moment we are trying to do classification. We will set our loss function to be the cross entropy.\n",
    "\n",
    "Torch provides the functionality to use generic functions as loss function. We will show an example one.\n",
    "\n",
    "Let's also reinstantiate the model, just because later you'll be asked to rerun the cells including the initialization of the model, and if we reinstantiate here you don't have to go too far above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfe6e4-082f-4f26-a70b-59d8dea0eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train_orig.shape[1], device)\n",
    "\n",
    "epochs=20\n",
    "learningRate = 0.001\n",
    "\n",
    "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
    "loss_fn = torch.nn.BCELoss() # If using the sigmoid. Otherwise, no sigmoid but: WithLogitsLoss(pos_weight=pos_weight.to(device)) (used to increase class importance for instance)\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19eec4-bf34-440b-ac76-14599477ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path = \"best_dnn_model.h5\"\n",
    "for t in range(epochs):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_orig, model, loss_fn, optimizer, scheduler, best_model_path, device, disable=True)\n",
    "    test_loss=test_loop(test_dataloader_orig, model, loss_fn, device, disable=True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c152d78-62b0-43f0-b64d-fcbabd238661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fb1e0-2a71-4061-86ad-d987040d98d4",
   "metadata": {},
   "source": [
    "What if we train more? Let's train for 40 more epochs.\n",
    "\n",
    "Now we have two choices: either we re-instantiate the model, increase the number of epochs, and retrain, or *we keep training from where we left off*!!! Let's try the latter, for another 40 epochs. Note how the loss function at the first step picks up the training from where it stopped before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748528c-fd7b-4cab-a911-ddb074f6443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(40):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_orig, model, loss_fn, optimizer, scheduler, best_model_path, device, True)\n",
    "    test_loss=test_loop(test_dataloader_orig, model, loss_fn, device, True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    #print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e78ecc-1e8b-4ec6-baf0-92c30e4a22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf98aa-860c-42a1-93b4-3d0fd0334108",
   "metadata": {},
   "source": [
    "Since we were appending the loss function to the vector of losses, the plot already shows all the epochs, and you can see that now we are training to convergence.\n",
    "\n",
    "Before plotting the ROC curves, let's retrain yesterday's adaptive boost BDT, as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931ef12-9f65-4dc2-8372-271d0ccb3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bdt_learning_rate = 0.1\n",
    "\n",
    "bdt_ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3, criterion='log_loss'), n_estimators=100, learning_rate=bdt_learning_rate, random_state=42)\n",
    "fitted_bdt_ada=bdt_ada.fit(X_train_orig, y_train_orig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150a788-f599-40c5-8d27-457d47c1786c",
   "metadata": {},
   "source": [
    "We can now plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b53d07-f43d-469a-9b99-c296e12cdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rocs(scores_labels_names):\n",
    "    plt.figure()\n",
    "    for score, label, name  in scores_labels_names:\n",
    "        fpr, tpr, thresholds = roc_curve(label, score)\n",
    "        plt.plot(\n",
    "            fpr, tpr, \n",
    "            linewidth=2, \n",
    "            label=f\"{name} (AUC = {100.*auc(fpr, tpr): .2f} %)\"\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.grid()\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "with torch.no_grad():\n",
    "    plot_rocs([\n",
    "        (fitted_bdt_ada.decision_function(X_test_orig), y_test_orig, 'AdaBoost (test)'),\n",
    "        (model(torch.tensor(X_train_orig.to_numpy(),device=model.device)).cpu().numpy(), y_train_orig, \"Train\"), \n",
    "        (model(torch.tensor(X_test_orig.to_numpy(),device=model.device)).cpu().numpy(), y_test_orig, \"Test\")  \n",
    "        # If using BCEWithLogitsLoss, then you need to apply the sigmoid by hand to get probabilities\n",
    "        #        (torch.sigmoid(model(torch.tensor(X_train_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_train_orig, \"Train\"), \n",
    "        #        (torch.sigmoid(model(torch.tensor(X_test_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_test_orig, \"Test\")  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66b3db-f38c-4e74-8a70-01383384b089",
   "metadata": {},
   "source": [
    "A 73.96% AUC is not bad, but also not particularly good, and AdaBoost is still overperforming, in particular for low FPR, which is usually our region of interest.\n",
    "\n",
    "Exercise:\n",
    "- The performance may improve if you weight more the positive class (to penalize misclassification). This is done by removing the sigmoid, switching to the `BCEWithLogitsLoss` with a custom weight (see commented code) and adding a sigmoid to the evaluation when computing the ROC curve (also commented code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247f24e-662d-4f4a-9d86-6f222d676669",
   "metadata": {},
   "source": [
    "However, note that we have used the unscaled features. What if we train a model to use the scaled features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20291f7-810d-4056-a871-7e6abd37fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled = NeuralNetwork(X_train_scaled.shape[1], device)\n",
    "\n",
    "epochs=20\n",
    "learningRate = 0.001\n",
    "\n",
    "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
    "loss_fn = torch.nn.BCELoss() #BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer_scaled = torch.optim.SGD(model_scaled.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler_scaled = torch.optim.lr_scheduler.ExponentialLR(optimizer_scaled, gamma=0.9)\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path_scaled = \"best_dnn_model_scaled.h5\"\n",
    "for t in range(epochs):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_scaled, model_scaled, loss_fn, optimizer_scaled, scheduler_scaled, best_model_path_scaled, device, disable=True)\n",
    "    test_loss=test_loop(test_dataloader_scaled, model_scaled, loss_fn, device, disable=True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plot_rocs([\n",
    "        (fitted_bdt_ada.decision_function(X_test_orig), y_test_orig, 'AdaBoost (test)'),\n",
    "        (model_scaled(torch.tensor(X_train_scaled.to_numpy(),device=model_scaled.device)).cpu().numpy(), y_train_scaled, \"Train\"), \n",
    "        (model_scaled(torch.tensor(X_test_scaled.to_numpy(),device=model_scaled.device)).cpu().numpy(), y_test_scaled, \"Test\")  \n",
    "        # If using BCEWithLogitsLoss, then you need to apply the sigmoid by hand to get probabilities\n",
    "        #(torch.sigmoid(model_scaled(torch.tensor(X_train_scaled.to_numpy(),device=model_scaled.device))).cpu().numpy().ravel(), y_train_scaled, \"Train\"), \n",
    "        #(torch.sigmoid(model_scaled(torch.tensor(X_test_scaled.to_numpy(),device=model_scaled.device))).cpu().numpy().ravel(), y_test_scaled, \"Test\")  \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a3a-2582-4f18-b93c-713258fcc380",
   "metadata": {},
   "source": [
    "Need to train more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d40f71-6380-44e1-8039-ee8586e53332",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(40):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_scaled, model_scaled, loss_fn, optimizer_scaled, scheduler_scaled, best_model_path_scaled, device, True)\n",
    "    test_loss=test_loop(test_dataloader_scaled, model_scaled, loss_fn, device, True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbbafe-cae9-4718-9919-e40852fad5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "with torch.no_grad():\n",
    "    plot_rocs([\n",
    "        (fitted_bdt_ada.decision_function(X_test_orig), y_test_orig, 'AdaBoost (test)'),\n",
    "        (model_scaled(torch.tensor(X_train_scaled.to_numpy(),device=model_scaled.device)).cpu().numpy(), y_train_scaled, \"Train\"), \n",
    "        (model_scaled(torch.tensor(X_test_scaled.to_numpy(),device=model_scaled.device)).cpu().numpy(), y_test_scaled, \"Test\")  \n",
    "        # If using BCEWithLogitsLoss, then you need to apply the sigmoid by hand to get probabilities\n",
    "        #(torch.sigmoid(model_scaled(torch.tensor(X_train_scaled.to_numpy(),device=model_scaled.device))).cpu().numpy().ravel(), y_train_scaled, \"Train\"), \n",
    "        #(torch.sigmoid(model_scaled(torch.tensor(X_test_scaled.to_numpy(),device=model_scaled.device))).cpu().numpy().ravel(), y_test_scaled, \"Test\")  \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de61aea-5b65-4dac-b590-1762c0b98f58",
   "metadata": {},
   "source": [
    "The AUC went down to 66.20 from 73.96% and the training is noisy.\n",
    "\n",
    "The scaling was supposed to have improved the classifier significantly!\n",
    "\n",
    "What is going on? Maybe it's a matter of which features are used?\n",
    "\n",
    "Let's check this by computing the permutation importance of the unscaled and scaled trainings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaccfb-7b81-4300-87e8-825a97afd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def compute_permutation_importance(model, X_test: pd.DataFrame, y_test: pd.Series, metric_fn=roc_auc_score, n_repeats=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate permutation feature importance based on AUC drop.\n",
    "    \"\"\"\n",
    "    # Convert inputs\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_array = y_test.values\n",
    "\n",
    "    # Baseline performance\n",
    "    baseline_preds = model(X_test_tensor).squeeze().cpu().detach().numpy()\n",
    "    baseline_score = metric_fn(y_test_array, baseline_preds)\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for col in X_test.columns:\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_test_permuted = X_test.copy()\n",
    "            X_test_permuted[col] = np.random.permutation(X_test_permuted[col].values)\n",
    "            X_perm_tensor = torch.tensor(X_test_permuted.values, dtype=torch.float32).to(device)\n",
    "            permuted_preds = model(X_perm_tensor).squeeze().cpu().detach().numpy()\n",
    "            permuted_score = metric_fn(y_test_array, permuted_preds)\n",
    "            scores.append(baseline_score - permuted_score)  # importance = drop in score\n",
    "        importances.append(np.mean(scores))\n",
    "\n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def plot_permutation_importance(importance_df, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot top_n most important features based on permutation importance.\n",
    "    \"\"\"\n",
    "    df = importance_df.copy()\n",
    "    df = df.sort_values(by=\"importance\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(5, 6))\n",
    "    plt.barh(df['feature'], df['importance'])\n",
    "    plt.xlabel(\"Importance (AUC Drop)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"Top {top_n} Feature Importances (Permutation)\")\n",
    "    plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "importance_df = compute_permutation_importance(model, X_test_orig, y_test_orig, metric_fn=roc_auc_score, n_repeats=10, device=device)\n",
    "plot_permutation_importance(importance_df)\n",
    "importance_df_scaled = compute_permutation_importance(model_scaled, X_test_scaled, y_test_scaled, metric_fn=roc_auc_score, n_repeats=10, device=device)\n",
    "plot_permutation_importance(importance_df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e8ab1-d5d4-4ed9-ba5f-63d889b0bc47",
   "metadata": {},
   "source": [
    "Now the situation is a bit more clear! Some of the irrelevant and low-magnitude features that were contributing the least to the unscaled classifier (for instance, the Hadronic top eta) now are super important and almost exclusively drive the output of the network!\n",
    "\n",
    "This can happen, and the interpretation is as follows: scaling brings all features to the same numerical footing. If some feature with smaller original magnitudes were also not important for the training (maybe they were injecting just noise, maybe they were very weak), then scaling them will amplify their influence: this is because features with small magnitudes are mostly ignored by neural networks unless they turn out to be so important that the network is forced to learn that they should have a very large weight. After scaling the feature, now the network may be sensitive to their noise because the noise is not penalized by the huge weight that was necessary to overcome the small average magnitude of the feature in the unscaled version.\n",
    "\n",
    "A typical workaround is to use the importance of the unscaled features to prune the least important variables. After that, scaling should actually result in an improved model: let's try and to that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bd714-b267-4013-80bf-896885c8260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "to_discard = importance_df[\"feature\"].tail(importance_df.shape[0]//3)\n",
    "\n",
    "X_train_orig = X_train_orig.drop(to_discard, axis=1)\n",
    "X_test_orig = X_test_orig.drop(to_discard, axis=1)\n",
    "X_train_scaled = X_train_scaled.drop(to_discard, axis=1)\n",
    "X_test_scaled = X_test_scaled.drop(to_discard, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Unscaled features\n",
    "train_dataset_orig = MyDataset(X_train_orig, y_train_orig, device=device)\n",
    "test_dataset_orig = MyDataset(X_test_orig, y_test_orig, device=device)\n",
    "\n",
    "train_dataloader_orig = DataLoader(train_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_orig = DataLoader(test_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Scaled features\n",
    "train_dataset_scaled = MyDataset(X_train_scaled, y_train_scaled, device=device)\n",
    "test_dataset_scaled = MyDataset(X_test_scaled, y_test_scaled, device=device)\n",
    "\n",
    "train_dataloader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52153116-61e6-4905-9644-7e4824822828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train_orig.shape[1], device)\n",
    "print(model)\n",
    "print(X_test_orig.shape)\n",
    "epochs=20\n",
    "learningRate = 0.001\n",
    "\n",
    "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
    "loss_fn = torch.nn.BCELoss() #BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path = \"best_dnn_model.h5\"\n",
    "for t in range(epochs):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_orig, model, loss_fn, optimizer, scheduler, best_model_path, device, disable=True)\n",
    "    test_loss=test_loop(test_dataloader_orig, model, loss_fn, device, disable=True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plot_rocs([\n",
    "        (model(torch.tensor(X_train_orig.to_numpy(),device=model.device)).cpu().numpy(), y_train_orig, \"Train\"), \n",
    "        (model(torch.tensor(X_test_orig.to_numpy(),device=model.device)).cpu().numpy(), y_test_orig, \"Test\")  \n",
    "        # If using BCEWithLogitsLoss, then you need to apply the sigmoid by hand to get probabilities\n",
    "        #(torch.sigmoid(model(torch.tensor(X_train_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_train_orig, \"Train\"), \n",
    "        #(torch.sigmoid(model(torch.tensor(X_test_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_test_orig, \"Test\")  \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac571237-9e61-4606-9a54-3a409d0a45b9",
   "metadata": {},
   "source": [
    "The ROC goes to 75.53 from 73.96: a very small price for having removed the least important input features.\n",
    "\n",
    "Let's see what happens to the scaled model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264936b-3785-40d4-bd0f-4ebe24034637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled = NeuralNetwork(X_train_scaled.shape[1], device)\n",
    "print(model_scaled)\n",
    "print(X_test_scaled.shape)\n",
    "epochs=60\n",
    "learningRate = 0.001\n",
    "\n",
    "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer_scaled = torch.optim.SGD(model_scaled.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler_scaled = torch.optim.lr_scheduler.ExponentialLR(optimizer_scaled, gamma=0.9)\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path_scaled = \"best_dnn_model_scaled.h5\"\n",
    "for t in range(epochs):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_scaled, model_scaled, loss_fn, optimizer_scaled, scheduler_scaled, best_model_path_scaled, device, disable=True)\n",
    "    test_loss=test_loop(test_dataloader_scaled, model_scaled, loss_fn, device, disable=True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler_scaled.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plot_rocs([\n",
    "    (model_scaled(torch.tensor(X_train_scaled.to_numpy(),device=model_scaled.device)).numpy(force=True), y_train_scaled, \"Train\"), \n",
    "    (model_scaled(torch.tensor(X_test_scaled.to_numpy(),device=model_scaled.device)).numpy(force=True), y_test_scaled, \"Test\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8e719-7c52-4479-aa62-dd9824f50138",
   "metadata": {},
   "source": [
    "Now the AUC is 67.39%, and it behaves a bit weirdly. What is likely happening is that the output is concentrated in some values and therefore the AUC jumps around: let's verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead4883-efd7-47d9-9d52-135fbc210917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.hist(model_scaled(torch.tensor(X_test_scaled[y_test_scaled==0].to_numpy(),device=model_scaled.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for background events\")\n",
    "plt.hist(model_scaled(torch.tensor(X_test_scaled[y_test_scaled==1].to_numpy(),device=model_scaled.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for signal events\")\n",
    "plt.title(\"With scaled features\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fed66e-ee6a-481f-8294-e666bf81cc09",
   "metadata": {},
   "source": [
    "Indeed we see a multimodal distribution, which is one of the worst cases. Compare with the output for the non-scaled features: it still recognizes some background events as signal events, but the double peak is less pronounced, and there is no signal peak at low classifier values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022977c-489e-4b99-ab44-6be127cd4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.hist(model(torch.tensor(X_test_orig[y_test_orig==0].to_numpy(),device=model.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for background events\")\n",
    "plt.hist(model(torch.tensor(X_test_orig[y_test_orig==1].to_numpy(),device=model.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for signal events\")\n",
    "plt.title(\"With unscaled features\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba40411-22a0-4543-aac0-060369bf55be",
   "metadata": {},
   "source": [
    "Let's also look at the feature importance now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccb364-69cb-462b-8f08-c9374c87ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importance_df = compute_permutation_importance(model, X_test_orig, y_test_orig, metric_fn=roc_auc_score, n_repeats=10, device=device)\n",
    "plot_permutation_importance(importance_df)\n",
    "importance_df_scaled = compute_permutation_importance(model_scaled, X_test_scaled, y_test_scaled, metric_fn=roc_auc_score, n_repeats=10, device=device)\n",
    "plot_permutation_importance(importance_df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2e898-c929-4838-bc86-28e596d23925",
   "metadata": {},
   "source": [
    "A-ha! Before scaling, the top variables are mostly masses and transverse momenta. After scaling, the single most important variable seems to be the phi angle of the `Hadronic top` variable, which doesn't make much sense.\n",
    "\n",
    "In machine learning, mostly you proceed guided by some loose principles, but most of the work is done by trial and error. Sometimes things that are supposed to help (scaling) are actually detrimental (they amplify noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fccd3-ea04-4f67-8492-15908082492b",
   "metadata": {},
   "source": [
    "What if we increase the amount of data and the size of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c92e6e-6536-4162-9ddc-e3fc4ea21a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"We have {len(X_train_orig)} training samples with {sum(y_train_orig)} signal and {sum(1-y_train_orig)} background events\")\n",
    "print(f\"We have {len(X_test_orig)} testing samples with {sum(y_test_orig)} signal and {sum(1-y_test_orig)} background events\")\n",
    "print(f\"Input shape is {X.shape}, target shape is {y.shape}.\")\n",
    "\n",
    "X_train_scaled = X_train_orig.copy(deep=True)\n",
    "X_test_scaled = X_test_orig.copy(deep=True)\n",
    "\n",
    "y_train_scaled = y_train_orig.copy(deep=True) # These won't change anyway, but to keep consistent names let's clone them too\n",
    "y_test_scaled = y_test_orig.copy(deep=True) # These won't change anyway, but to keep consistent names let's clone them too\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler, # maxAbs\n",
    "    MinMaxScaler, # MinMax\n",
    "    Normalizer, # Normalization (equal integral)\n",
    "    StandardScaler# standard scaling\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the input features and the target variable\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_scaled)\n",
    "X_train_scaled[X_train_orig.columns] = scaler.transform(X_train_scaled[X_train_orig.columns])\n",
    "X_test_scaled[X_train_orig.columns] = scaler.transform(X_test_scaled[X_train_orig.columns])\n",
    "#for column in X_train_scaled.columns:\n",
    "#    scaler = StandardScaler().fit(X_train_scaled.filter([column], axis=1))\n",
    "#    X_train_scaled[column] = scaler.transform(X_train_scaled.filter([column], axis=1))\n",
    "#    X_test_scaled[column] = scaler.transform(X_test_scaled.filter([column], axis=1))\n",
    "\n",
    "# Unscaled features\n",
    "train_dataset_orig = MyDataset(X_train_orig, y_train_orig, device=device)\n",
    "test_dataset_orig = MyDataset(X_test_orig, y_test_orig, device=device)\n",
    "\n",
    "train_dataloader_orig = DataLoader(train_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_orig = DataLoader(test_dataset_orig, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Scaled features\n",
    "train_dataset_scaled = MyDataset(X_train_scaled, y_train_scaled, device=device)\n",
    "test_dataset_scaled = MyDataset(X_test_scaled, y_test_scaled, device=device)\n",
    "\n",
    "train_dataloader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now we don't subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee70dfd-5de7-4133-8196-d40bcbbee78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ninputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid() # No sigmoid if using BCEWithLogitsLoss\n",
    "        )\n",
    "        self.linear_relu_stack.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabd677-1c77-48d0-b140-5696258f3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train_orig.shape[1], device)\n",
    "print(model)\n",
    "print(X_test_orig.shape)\n",
    "epochs=20\n",
    "learningRate = 0.001\n",
    "\n",
    "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
    "loss_fn = torch.nn.BCELoss() #BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path = \"best_dnn_model.h5\"\n",
    "for t in range(epochs):\n",
    "    if t%5 == 0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader_orig, model, loss_fn, optimizer, scheduler, best_model_path, device, disable=True)\n",
    "    test_loss=test_loop(test_dataloader_orig, model, loss_fn, device, disable=True)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if t%5 == 0:\n",
    "        print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plot_rocs([\n",
    "        (model(torch.tensor(X_train_orig.to_numpy(),device=model.device)).cpu().numpy(), y_train_orig, \"Train\"), \n",
    "        (model(torch.tensor(X_test_orig.to_numpy(),device=model.device)).cpu().numpy(), y_test_orig, \"Test\")  \n",
    "        # If using BCEWithLogitsLoss, then you need to apply the sigmoid by hand to get probabilities\n",
    "        #(torch.sigmoid(model(torch.tensor(X_train_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_train_orig, \"Train\"), \n",
    "        #(torch.sigmoid(model(torch.tensor(X_test_orig.to_numpy(),device=model.device))).cpu().numpy().ravel(), y_test_orig, \"Test\")  \n",
    "\n",
    "    ])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(model(torch.tensor(X_test_orig[y_test_orig==0].to_numpy(),device=model.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for background events\")\n",
    "plt.hist(model(torch.tensor(X_test_orig[y_test_orig==1].to_numpy(),device=model.device)).numpy(force=True), bins=20, alpha=0.7, label=\"Score for signal events\")\n",
    "plt.title(\"With unscaled features\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f95be0-b70a-4a95-960e-a9e52d4e9996",
   "metadata": {},
   "source": [
    "Wow! We gained a full 2%, to 72.03!!!\n",
    "\n",
    "Let's look at the amount of training data and the amount of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798eb182-3839-4b51-8859-a532466e363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data size: \", X_train_orig.shape[0])\n",
    "torchinfo.summary(model, input_size=(batch_size, X_train_orig.shape[1])) # the input size is (batch size, number of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f1579-0c77-420c-a047-3b7ac19172ec",
   "metadata": {},
   "source": [
    "We have 201809 parameters for 263137 data points.\n",
    "\n",
    "Exercise:\n",
    "- Increase obscenely the network structure, to maybe 300k parameters, and retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece5d9b-d183-4a42-9860-ffcde9f8f1bf",
   "metadata": {},
   "source": [
    "In any case, this case is pretty difficult to handle: the `bk1`, ttW production, is very similar to the signal ttH, so clumping ttW together with Drell-Yan in training can easily confuse the network: indeed, the final classifier recognizes that there are two regimes in the background, one that peaks near the signal (is similar to the signal) and the other that doesn't.\n",
    "\n",
    "Next: let's try a multiclass separation between signal, bk1, bkg2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ead6e-9217-4e63-ba7f-11c6da6233b7",
   "metadata": {},
   "source": [
    "## That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34140a-06b4-4d5f-a0de-4ebd2269f946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
