{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de Datos en Física Moderna\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n",
    "\n",
    "The core of this tutorial comes from https://github.com/vischia/data_science_school_igfae2024 (Pietro Vischia (pietro.vischia@cern.ch)).\n",
    "\n",
    "\n",
    "This tutorial is based on Andrey Karpathy's model [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
    "\n",
    "Data are downloaded from [Amephraim's repository](https://github.com/amephraim/nlp/tree/master/texts), forked to [vischia/nlp_datasets](https://github.com/vischia/nlp_datasets) for data persistance reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](figs/all_models.png)\n",
    "\n",
    "We'll implement a decoder-only structure (figure adapted by [Bruno Maga](https://brunomaga.github.io/GPT-lite) from the paper [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "- Left: the original transformer structure with an encoder and a decoder block.\n",
    "- Middle: a single-block decoder-only model architecture.\n",
    "- Right: the multi-block GPT decoder-only architecture, detailed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the random seed (for reproducibility)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# If you have a MAC, use the following line. If you don't have a MAC, comment it out.\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global training parameters\n",
    "# How often to do an evaluation step\n",
    "eval_interval = 100\n",
    "# Number of training iterations\n",
    "max_iters = 500\n",
    "# Optimizer's learning rate\n",
    "learning_rate=1e-4\n",
    "# Minibatch size\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "### GPT's structural parameters\n",
    "# The maximum sequence length used as input.\n",
    "# E.g. for block_size 4 and input ABCD, we have training samples A->B, AB->C, ABC->C, ABCD->E\n",
    "block_size = 4\n",
    "\n",
    "# Size of the embeddings\n",
    "n_embd = 12\n",
    "\n",
    "# Number of attention heads in Multi-Attention mechanism (the `Nx` in the GPT decoder diagram)\n",
    "n_head = 6\n",
    "\n",
    "# Depth of the network as number of decoder blocks.\n",
    "# Each block contains a normalization, an attention and a feed forward unit\n",
    "n_layer = 6\n",
    "\n",
    "# Dropout rate (variable p) for dropout units\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the first time you run. Comment immediately thereafter, to avoid downloading them multiple times\n",
    "import os\n",
    "hp_1=\"./JKRowling_HarryPotter1_SorcerersStone.txt\"\n",
    "hp_2=\"./JKRowling_HarryPotter2_TheChamberOfSecrets.txt\"\n",
    "hp_3=\"./JKRowling_HarryPotter3_PrisonerOfAzkaban.txt\"\n",
    "hp_4=\"./JKRowling_HarryPotter4_TheGobletOfFire.txt\"\n",
    "\n",
    "if not os.path.isfile(hp_1):\n",
    "    !wget https://raw.githubusercontent.com/vischia/nlp_datasets/refs/heads/master/texts/JKRowling_HarryPotter1_SorcerersStone.txt\n",
    "if not os.path.isfile(hp_2):\n",
    "    !wget https://raw.githubusercontent.com/vischia/nlp_datasets/refs/heads/master/texts/JKRowling_HarryPotter2_TheChamberOfSecrets.txt\n",
    "if not os.path.isfile(hp_1):\n",
    "    !wget https://raw.githubusercontent.com/vischia/nlp_datasets/refs/heads/master/texts/JKRowling_HarryPotter3_PrisonerOfAzkaban.txt\n",
    "if not os.path.isfile(hp_1):\n",
    "    !wget https://raw.githubusercontent.com/vischia/nlp_datasets/refs/heads/master/texts/JKRowling_HarryPotter4_TheGobletOfFire.txt\n",
    "\n",
    "\n",
    "# Let's start by training on the first Harry Potter book\n",
    "with open(hp_1) as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "We need to map characters to tokens\n",
    "The simplest way is to map characters to integers directly.\n",
    "\n",
    "First let's use this. In the tasks below, you will be requested to try another encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sorted list of input characters and create \n",
    "# string-to-int (stoi) and int-to-string (itos) representations:\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# define encode and decode functions that convert strings to arrays of tokens and vice-versa\n",
    "encode = lambda x: torch.tensor([stoi[ch] for ch in x], dtype=torch.long) #encode text to integers\n",
    "decode = lambda x: ''.join([itos[i] for i in x]) #decode integers to text\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(\"The vocabulary will have\", vocab_size, \"unique elements\")\n",
    "print(\"Full vocabulary:\", stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encoding (embedding)\n",
    "\n",
    "We need to introduce the notion of sequence, i.e. the ordering and mutual relationship between words. If we don't, we would have to stick with the set of vectors returned by the Attention mechanism, with no sequence structure.\n",
    "\n",
    "In the original paper, the authors use use a positional embedding based on the and frequencies.\n",
    "Here, for simplicity we will use the `Embedding` function of pytorch, which encodes to [a dictionary of fixed size](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)    # from tokens to embedding\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd) # from position to embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text)  # This would be the place where you use the alternative encoding mentioned below\n",
    "\n",
    "# Have 90% data for training and 10% for validation.\n",
    "n = int(0.9*len(data))\n",
    "train_data, valid_data = data[:n], data[n:]\n",
    "print(\"Training data set size:\", len(train_data), \"\\nTest data set size:\", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "\n",
    "This is an extension of the usual batching we have used in the first tutorials: the batch size is not fixed!!!\n",
    "What is fixed (governed by the parameter `block_size` defined above) is the maximum size of each batch element, i.e. the maximum length of each sequence included in the batch.\n",
    "\n",
    "We therefore need, for each input, all sequencies from size `1 `to size `block_size`. For each input sequence from position `0` to `t`, the respective output is given by the element in the position `t+1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source):\n",
    "    \"\"\" get batch of size block_size from source \"\"\"\n",
    "    # Generate `batch_size` random offsets on the data \n",
    "    ix = torch.randint(len(source)-block_size, (batch_size,) )\n",
    "    # Collect `batch_size` subsequences of length `block_size` from source, as data and target\n",
    "    x = torch.stack([source[i:i+block_size] for i in ix])\n",
    "    # Target is just x shifted right (ie the predicted token is the next in the sequence)\n",
    "    y = torch.stack([source[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# Test it out\n",
    "xb, yb = get_batch(train_data)\n",
    "print(\"Input:\\n\",xb)\n",
    "print(\"Target:\\n\",yb)\n",
    "\n",
    "for b in range(batch_size): #for every batches\n",
    "    print(f\"\\n=== batch {b}:\")\n",
    "    for t in range(block_size): #for each sequence in block\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"for input {context.tolist()} target is {target.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention, step by step\n",
    "\n",
    "Multi-headed attention: this is an extension of the Attention.\n",
    "The attention we have seen in class encodes patterns of attention between elements. There may however be more than one pattern that is relevant at the same time. Using one attention head (the Attention we have seen in class) can average out these effects. We can use multiple heads, each with its own independent trainable parameters, and let the network learn these different patterns (up to `n_heads` different patterns). It's the same we have seen in the CNN tutorial, where multiple filters at the same time made it possible to focus on different features of the image.\n",
    "\n",
    "We define MultiHead attention as:\n",
    "$$\n",
    "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } Attention(Q,K,V) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) \\, V\n",
    "$$\n",
    "\n",
    "\n",
    "Let's start with the $$W^Q$$, $$W^K$$ and $$W^V$$ matrices are computed as a simple projection (*linear layer*): this corresponds to what in class we have seen to be *\"the way of making the elements learnable is to introduce linear combinations with weights, and the way of making key, query, and value be learned separately is to have three different matrices*\n",
    "\n",
    "```python\n",
    "head_size=4\n",
    "key   = nn.Linear(C, head_size, bias=False) \n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "```\n",
    "\n",
    "We can now compute the $$Attention(Q,K,V)$$ as:\n",
    "\n",
    "```python\n",
    "k = key(x) #shape (B,T, head_size)\n",
    "q = query(x) #shape (B,T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) = (B,T,T)\n",
    "wei *= head_size**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1\n",
    "```\n",
    "\n",
    "We then adapt the (alternative) notation of the uniform attention above, and compute the output of the non-uniform attention matrix as:\n",
    "```python\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) #tokens only \"talk\" to previous tokens\n",
    "wei = F.softmax(wei, dim=-1) #equivalent to the normalization above (-inf in upper diagonal will be 0)\n",
    "v = value(x) # shape (B,T, head_size)\n",
    "out = wei @ v # shape (B,T,T) @ (B,T,C) --> (B,T,C)\n",
    "```\n",
    "\n",
    "Note that `out = wei @ x` is the same inner dot-product of the previous items, but this time the attention weights are not uniform, they are learnt parameters and change per query and over time. And **this is the main property and the main problem that the self-attention solves: non-uniform attention weights per query**. This is different than the uniform attention matrix where weights were uniform across all previous tokens, i.e. aggregation was just a raw average of all tokens in the sequence. Here we aggregate them by a \"value of importance\" for each token.\n",
    "\n",
    "Also, without the $\\sqrt{d_k}$ normalisation, we would have diffused values in `wei`, and it would approximate a one-hot vector. This normalization creates a more sparse `wei` vector.\n",
    "\n",
    "This mechanism we coded is called *self-attention* because the $K$, $Q$ and $V$ all come from the same input `x`. But attention is more general. `x` can be given by a data source, and $K$, $Q$ and $V$ may come from different sources -- this would be called *cross attention*.\n",
    "\n",
    "As final remarks, note that elements across batches are always independent, i.e. no cross-batch attention. And in many cases, e.g. a string representation of chemical compounds, or sentiment analysis, there can be no attention mask (i.e. all tokens can attend to all tokens), or there's a custom mask that fits the use case (e.g. main upper and lower diagonals to allow tokens to see their closest neighbour only). And here, we also don't have any cross atttention between the encoder and decoder.\n",
    "\n",
    "The decoder includes a multi-head attention, which is simply a concatenation of individual heads' outputs. The `Head` and `MultiHeadAttention` modules can then be implemented as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #Note: this dropout randomly prevents some tokens from communicating with each other\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #shape (B,T, head_size)\n",
    "        q = self.query(x) #shape (B,T, head_size)\n",
    "        v = self.value(x) #shape (B,T, head_size)\n",
    "\n",
    "        #compute self-attention scores\n",
    "        wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) --> (B,T,T)\n",
    "        wei *= C**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        #perform weighted aggregation of values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head attention as a collection of heads with concatenated outputs.\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed-forward layer\n",
    "\n",
    "Attention weights will induce a nonlinearity via the softmax function, but the transformer transforms inputs into outputs in a same space, and this may limit the expressivity of the block. To enhance flexibility, one typically plugs at least one standard NN layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" the feed forward network (FFN) in the paper\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # Note: in the paper (section 3.3) we have d_{model}=512 and d_{ff}=2048.\n",
    "        # Therefore the inner layer is 4 times the size of the embedding layer\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "          )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GPT block\n",
    "\n",
    "The standard block is composed by a MultiHead attention followed by a feed-forward layer.\n",
    "\n",
    "Because the network can become too deep (and hard to train) for a high number of sequential blocks, we added skip connections to each block. Also, in the original paper, the layer normalization operation is applied after the attention and the feed-forward network, but before the skip connection. In modern days, it is common to apply it in the pre-norm formulation, where normalization is applied before the attention and the FFN. That’s also what we’ll do in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: comunication (attention) followed by computation (FFN) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        # n_heads : the number of heads we'd like to use\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full GPT network\n",
    "\n",
    "The `__init__` and `forward` constitute the main model, the `generate` is the function we will use for inference (in this case, inference means *\"generate new text*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTlite(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "    \n",
    "        # vocabulary embedding and positional embedding\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        #sequence of attention heads and feed forward layers\n",
    "        self.blocks = nn.Sequential( *[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "\n",
    "        #one layer normalization layer after transformer blocks\n",
    "        #and one before linear layer that outputs the vocabulary\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\" call the model with idx and targets (training) or without targets (generation)\"\"\"\n",
    "\n",
    "        #idx and targets are both of shape (B,T)\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) #shape (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) #shape (T,C)\n",
    "        x = tok_emb + pos_emb #shape (B,T,C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.lm_head(x) #shape (B,T,C)\n",
    "        logits = torch.swapaxes(logits, 1, 2) #shape (B,C,T) to comply with CrossEntropyLoss\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" given a context idx, generate max_new_tokens tokens and append them to idx \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] #we can never have any idx longer than block_size\n",
    "            logits = self(idx_cond) #call fwd without targets\n",
    "            logits = logits[:, :, -1] # take last token. from shape (B, C, T) to (B, C)\n",
    "            #convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # shape (B, C)\n",
    "            #randomly sample the next tokens, 1 for each of the previous probability distributions\n",
    "            #(one could take instead the argmax, but that would be deterministic and boring)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape (B, 1)\n",
    "            #append next token ix to the solution sequence so far\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # shape (B, T+1)\n",
    "        return idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m  = GPTlite(vocab_size).to(device)\n",
    "\n",
    "# train the model\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "for steps in range(max_iters):\n",
    "    idx, targets = get_batch(train_data)   #get a batch of training data\n",
    "    logits = m(idx)   #forward pass\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    loss.backward()   #backward pass\n",
    "    optimizer.step()   #update parameters\n",
    "    optimizer.zero_grad(set_to_none=True)  #sets to None instead of 0, to save memory\n",
    "\n",
    "    #print progress\n",
    "    if steps % 100 == 0: print(f\"step {steps}, loss {loss.item():.2f}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    # eval loop: no backprop on this data, to avoid storing all intermediatte variables\n",
    "    def eval_loss():\n",
    "        idx, targets = get_batch(valid_data)   #get a batch of validation data\n",
    "        logits = m(idx)   #forward pass\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        print(f\"step {steps}, eval loss {loss.item():.2f}\")\n",
    "        return loss\n",
    "  \n",
    "    if steps % eval_interval == 0: eval_loss().item()\n",
    "\n",
    "        \n",
    "# You may want to save the model here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Particularly for the tasks where you'll have to play with inference with different inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference: generate text\n",
    "\n",
    "We pass a single token (the `\\n` character, encoded as token `0`) to the model as initial character, and let it generate a sequence of 500 tokens.\n",
    "\n",
    "In the tasks below, you will be requested to play by passing different tokens, or sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a 1x1 tensor with batch size 1 and sequence length 1 and starting value 0 (0 is the \\n character)\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# test the same generate() function, now with the trained model\n",
    "print(decode(m.generate(idx, max_new_tokens=500).tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the inference step, try to pass a different initial token or a sequence of tokens. Remember that the sequence cannot be longer than the block size: it will be chopped of within the decode function. Try this with character-by-character tokenization, and with `tiktoken` tokenization. For word-based tokenization or for large `block_size` character-based tokenization, what happens if the input sequence is related to the book theme opposed to if it isn't (e.g. another theme, *\"Elon Musk announced a new Tesla truck\"* or another language, *\"El Alimerka no tenía más pan\"*)\n",
    "```python\n",
    "testsentence= encode(\"Harry Potter\")\n",
    "testsentence = testsentence.reshape((len(testsentence),1)).to(device)\n",
    "print(decode(m.generate(testsentence, max_new_tokens=500).tolist()[0]))\n",
    "```\n",
    "\n",
    "2. In the encoding step, try to use a different encoding scheme. For instance, you can use the *Byte-Pair-Encoding (BPE)* scheme by OpenAI's GPT, called `tiktoken`\n",
    "```python\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode(\"Hello world\"))\n",
    "data_enc = enc.encode(text)\n",
    "print(data_enc)\n",
    "```\n",
    "\n",
    "3. Try to change structural and training parameters. Which parameter(s) influence the most the quality (in terms of how much it resembles an actual text) of the generated text? Can you think of a reason why a certain parameter is the most impactful, particularly in the character-by-character encoding? NOTE: you will typically need to increase training to 1000 or a few thousand `max_iter`. Also , don't forget to train on GPU (in `colab` or in your laptop with GPU card) or on MPS devices (MAC M1/M2/M3 laptops).\n",
    "```python\n",
    "eval_interval = 100\n",
    "max_iters = 5000\n",
    "learning_rate=3e-4\n",
    "batch_size = 128\n",
    "block_size = 256\n",
    "n_embd = 300\n",
    "n_layer = 10\n",
    "dropout = 0.2\n",
    "```\n",
    "\n",
    "4. What happens if you enlarge the training dataset (e.g. including the other three books, or one or two of them)? Do you see any change in performance, both if you use the character-by-characted tokenization or the `tiktoken` tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsentence= encode(\"Harry Potter\")\n",
    "print(testsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode(\"Harry Potter\"))\n",
    "#data_enc = enc.encode(text)\n",
    "\n",
    "a = enc.encode(\"Harry Potter\")\n",
    "print(a)\n",
    "b = enc.decode(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
